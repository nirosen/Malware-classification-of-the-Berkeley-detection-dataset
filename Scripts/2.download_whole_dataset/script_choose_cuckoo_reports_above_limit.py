import itertools
import multiprocessing as mp
import csv
import sys
import argparse
import os
import json
import pickle
import datetime
import shutil
import datetime
import multiprocessing
import time
import logging
from pathlib import Path
from symbol import continue_stmt

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
output_dir_path_Path = Path("/home/john/Desktop/git/Malware-classification-of-the-Berkeley-detection-dataset/Scripts/2.download_whole_dataset/cuckoo1M_reports_above_limit/")
cuckoo_reports_dir_Path = Path("/run/media/john/5TBsata/miller/1M_vt_cuckoo/cuckoo1M_above_limit_556456")
limit_reports_files = None
minimum_syscalls_length = 500





def process_reports_file(dict_manager, file_name):
    try:
        with open(file=cuckoo_reports_dir_Path/file_name, mode='rb') as file_handle:
                # 1. process report file to json
                cuckoo_report = json.load(file_handle)

                # 2. count syscalls from all processes:
                procs_list = cuckoo_report['behavior']['processes']
                calls_count = 0
                for proc_dict in procs_list:
                    calls_count += len(proc_dict["calls"])
                    if calls_count >= minimum_syscalls_length:
                        # dict_manager.append(file_name)
                        return
        (cuckoo_reports_dir_Path/file_name).unlink()
        print('removed: '+file_name)
                        
    except json.JSONDecodeError as err:
        # TODO: (cuckoo_reports_dir_Path/file_name).unlink()   # print('removed: '+file_name)
        (cuckoo_reports_dir_Path/file_name).unlink()
        print('removed: '+file_name)
        pass
    except:
        # TODO: (cuckoo_reports_dir_Path/file_name).unlink()   # print('removed: '+file_name)
        (cuckoo_reports_dir_Path/file_name).unlink()
        print('removed: '+file_name)        
        pass


def main():
    if not os.path.exists(output_dir_path_Path):
        os.makedirs(output_dir_path_Path)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s",
        handlers=[
            logging.FileHandler(os.path.join(output_dir_path_Path, "log.txt")),
            logging.StreamHandler(sys.stdout)
        ])
    logger = logging.getLogger()
    logger.info("main func start, print arguments for this run:")


    # dict = {}
    # file_name = "eeda4f05ff83ee1dcfc86316e91d2ce69452e93bf1b1c604e50de891c21e87cc_report.json"
    # process_reports_file(dict, file_name)
    # print(dict)

    with multiprocessing.Manager() as manager:
        dict_manager = manager.list()
        cpu_cores = 1
        if sys.platform == 'linux':
            # check avail cpu cores - only-Linux!
            cpu_cores = len(os.sched_getaffinity(0))-2
        else:
            # check CPU cores number - multi-platform
            cpu_cores = multiprocessing.cpu_count()-2
        starttime = time.time()
        num_processes = cpu_cores
        multiprocessing.freeze_support()
        with multiprocessing.Pool(num_processes) as pool:

            try:
                map_func = pool.apply_async
                a_results = [map_func(func=process_reports_file, args=(dict_manager, file_name))
                             for file_name in
                             itertools.islice(os.listdir(cuckoo_reports_dir_Path), 0, limit_reports_files)]
                pool.close()
                pool.join()
            except:
                logger.info("---- main loop error ----")
                logger.info(sys.exc_info)
                sys.exit(1)


        logger.info('That took {} seconds'.format(time.time() - starttime))
        dict_manager_file_path = os.path.join(output_dir_path_Path, "files_list_"+str(time.time())+".pickle")
                
        with open(dict_manager_file_path, 'wb') as output_file:
            pickle.dump(dict_manager.__deepcopy__({}), output_file)  # dict_manager.__deepcopy__({})
            
        
    
    
        logger.info("finished run, resulted dict in: " + dict_manager_file_path)

if __name__ == '__main__':
    try:
        main()
        import os
        #os.system("shutdown now -h")
    except Exception as ex:
        print(ex)
        import os
        #os.system("shutdown now -h")
        sys.exit(1)

        

#dict_manager_file_path = '/home/john/Desktop/git/Malware-classification-of-the-Berkeley-detection-dataset/Scripts/2.download_whole_dataset/cuckoo1M_reports_above_limit/files_list_1601062445.4677486.pickle'
#with open(dict_manager_file_path, 'rb') as pickle_file:
#            list = pickle.load(pickle_file)
#            print(len(list))
#            print(list[0])
#            print(list[1])
#            print(list)
        
        
        
        
        
        
#         dict_manager_file_path = os.path.join(output_dir_path_Path, "dict.json"+str(time.time()))
#         with open(dict_manager_file_path, 'w') as output_file:
#             json_dict = json.dumps(dict_manager.copy(), ensure_ascii=False)
#             output_file.write(dict_manager)  # .encode("utf-16"))

        
        
        
        
        
        
        
        
        
        
# def process_reports_file(dict_manager, file_name, logger):
#     try:
#         lines = open(file=cuckoo_reports_dir_Path / file_name, mode='rb').readlines()
#         for line in lines:
#             try:
#                 cuckoo_report = json.loads(line.decode(encoding="UTF-8", errors='replace'))
#                 max_calls = max(len(pid_dict['calls']) for pid_dict in cuckoo_report['behavior']['processes'])
#                 if limit_syscalls_length > max_calls:
#                     continue
#                 sha256 = cuckoo_report['sha256']
#                 syscalls_file_Path = cuckoo_reports_dir_Path / "flatten" / sha256
#                 if os.path.isfile(syscalls_file_Path):
#                     continue
#                 with open(file=syscalls_file_Path, mode='w', encoding="utf8") as syscalls_file:
#                      json.dump(cuckoo_report, syscalls_file)
#             except json.JSONDecodeError as err:
#                 print("process report {0}   JSONDecodeError error: {1}".format(sha256, err))
#                 pass
#             except:
#                 print("process_reports_file: {0}, error: {1}".format(file_name, sys.exc_info()))
#                 pass
#     except IOError as err:
#         print("process_reports_file   I/O error: {0}".format(err))
#     except:
#         print("process_reports_file: {0}, error: {1}".format(file_name, sys.exc_info()))


# def process_reports_file(dict_manager, file_name):
#     try:
#         with open(file=cuckoo_reports_dir_Path/file_name, mode='rb') as file_handle:
#                 # 1. process report file to json
#                 cuckoo_report = json.load(file_handle)

#                 # 2. count syscalls from all processes:
#                 procs_list = cuckoo_report['behavior']['processes']
#                 calls_count = 0
#                 for proc_dict in procs_list:
#                     calls_count += len(proc_dict["calls"])

#                 # 3. copy the reports which are above the minimum syscalls limit
#                 if minimum_syscalls_length <= calls_count:
#                     # case of report which is above the minimum syscalls limit
#                     ## sha256 = cuckoo_report['sha256']    # report doesnt contains sha256 - why???
#                     dst_file_Path = output_dir_path_Path / file_name
#                     if os.path.isfile(dst_file_Path):
#                         dict_manager[file_name] = "dst file already exists."
#                         return
#                     with open(file=dst_file_Path, mode='w', encoding="utf8") as dst_file:
#                          json.dump(cuckoo_report, dst_file)
#                 else:
#                     # case of report which is below the minimum syscalls limit
#                     return

#     except json.JSONDecodeError as err:
#         dict_manager[file_name] = "JSONDecodeError error."
#         print("process report {0}   JSONDecodeError error: {1}".format(file_name, err))
#         pass
#     except:
#         dict_manager[file_name] = "some error."
#         print("process_reports_file: {0}, error: {1}".format(file_name, sys.exc_info()))
#         pass








'''
def process_reports_file(dict_manager, file_name):
    try:
        with open(file=cuckoo_reports_dir_Path/file_name, mode='rb') as file_handle:
                # 1. process report file to json
                cuckoo_report = json.load(file_handle)

                # 2. count syscalls from all processes:
                procs_list = cuckoo_report['behavior']['processes']
                calls_count = 0
                for proc_dict in procs_list:
                    calls_count += len(proc_dict["calls"])
                    if calls_count >= minimum_syscalls_length:
                        # dict_manager.append(file_name)
                        return
        (cuckoo_reports_dir_Path/file_name).unlink()
        #print('removed: '+file_name)
                        
    except json.JSONDecodeError as err:
        pass
    except:
        pass


def main():
    if not os.path.exists(output_dir_path_Path):
        os.makedirs(output_dir_path_Path)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s",
        handlers=[
            logging.FileHandler(os.path.join(output_dir_path_Path, "log.txt")),
            logging.StreamHandler(sys.stdout)
        ])
    logger = logging.getLogger()
    logger.info("main func start, print arguments for this run:")


    # dict = {}
    # file_name = "eeda4f05ff83ee1dcfc86316e91d2ce69452e93bf1b1c604e50de891c21e87cc_report.json"
    # process_reports_file(dict, file_name)
    # print(dict)

    with multiprocessing.Manager() as manager:
        dict_manager = manager.list()
        cpu_cores = 1
        if sys.platform == 'linux':
            # check avail cpu cores - only-Linux!
            cpu_cores = len(os.sched_getaffinity(0))-2
        else:
            # check CPU cores number - multi-platform
            cpu_cores = multiprocessing.cpu_count()-2
        starttime = time.time()
        num_processes = cpu_cores
        multiprocessing.freeze_support()
        with multiprocessing.Pool(num_processes) as pool:

            try:
                map_func = pool.apply_async
                a_results = [map_func(func=process_reports_file, args=(dict_manager, file_name))
                             for file_name in
                             itertools.islice(os.listdir(cuckoo_reports_dir_Path), 0, limit_reports_files)]
                pool.close()
                pool.join()
            except:
                logger.info("---- main loop error ----")
                logger.info(sys.exc_info)
                sys.exit(1)


        logger.info('That took {} seconds'.format(time.time() - starttime))
        dict_manager_file_path = os.path.join(output_dir_path_Path, "files_list_"+str(time.time())+".pickle")
                
        with open(dict_manager_file_path, 'wb') as output_file:
            pickle.dump(dict_manager.__deepcopy__({}), output_file)  # dict_manager.__deepcopy__({})
            
        
    
    
        logger.info("finished run, resulted dict in: " + dict_manager_file_path)

if __name__ == '__main__':
    try:
        main()
	import os
        os.system("shutdown now -h")
    except Exception as ex:
        print(ex)
	import os
        os.system("shutdown now -h")
        sys.exit(1)
'''





'''

    
def process_reports_file(file_name):
    try:
        with open(file=cuckoo_reports_dir_Path/file_name, mode='rb') as file_handle:
                # 1. process report file to json
                cuckoo_report = json.load(file_handle)

                # 2. count syscalls from all processes:
                procs_list = cuckoo_report['behavior']['processes']
                calls_count = 0
                for proc_dict in procs_list:
                    calls_count += len(proc_dict["calls"])
                    if calls_count >= minimum_syscalls_length:
                        # dict_manager.append(file_name)
                        return
        (cuckoo_reports_dir_Path/file_name).unlink()
        print('removed: '+file_name)
                        
    except json.JSONDecodeError as err:
        pass
    except Exception as ex:
        print(ex)
        pass


def main():
    if not os.path.exists(output_dir_path_Path):
        os.makedirs(output_dir_path_Path)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s",
        handlers=[
            logging.FileHandler(os.path.join(output_dir_path_Path, "log.txt")),
            logging.StreamHandler(sys.stdout)
        ])
    logger = logging.getLogger()
    logger.info("main func start, print arguments for this run:")


    # dict = {}
    # file_name = "eeda4f05ff83ee1dcfc86316e91d2ce69452e93bf1b1c604e50de891c21e87cc_report.json"
    # process_reports_file(dict, file_name)
    # print(dict)

    with multiprocessing.Manager() as manager:
        #dict_manager = manager.list()
        cpu_cores = 1
        if sys.platform == 'linux':
            # check avail cpu cores - only-Linux!
            cpu_cores = len(os.sched_getaffinity(0))-2
        else:
            # check CPU cores number - multi-platform
            cpu_cores = multiprocessing.cpu_count()-2
        starttime = time.time()
        num_processes = cpu_cores
        multiprocessing.freeze_support()
        with multiprocessing.Pool(num_processes) as pool:

            try:
                map_func = pool.apply_async
                a_results = [map_func(func=process_reports_file, args=(file_name))
                             for file_name in
                             itertools.islice(os.listdir(cuckoo_reports_dir_Path), 0, limit_reports_files)]
                pool.close()
                pool.join()
            except:
                logger.info("---- main loop error ----")
                logger.info(sys.exc_info)
                sys.exit(1)


        logger.info('That took {} seconds'.format(time.time() - starttime))
        #dict_manager_file_path = os.path.join(output_dir_path_Path, "files_list_"+str(time.time())+".pickle")
                
        #with open(dict_manager_file_path, 'wb') as output_file:
            #pickle.dump(dict_manager.__deepcopy__({}), output_file)  # dict_manager.__deepcopy__({})
            
        
    
    
        #logger.info("finished run, resulted dict in: " + dict_manager_file_path)
'''
