import itertools
import multiprocessing as mp
import csv
import sys
import argparse
import os
import json
import pickle
import datetime
import shutil
import datetime
import multiprocessing
import time
import logging
from pathlib import Path
from symbol import continue_stmt

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
output_dir_path_Path = Path("/run/media/john/New Volume/miller_406993_cuckoo7July/reports_above_limit")
cuckoo_reports_dir_Path = Path("/run/media/john/New Volume/miller_406993_cuckoo7July/reports")
limit_reports_files = None
minimum_syscalls_length = 500


# def process_reports_file(dict_manager, file_name, logger):
#     try:
#         lines = open(file=cuckoo_reports_dir_Path / file_name, mode='rb').readlines()
#         for line in lines:
#             try:
#                 cuckoo_report = json.loads(line.decode(encoding="UTF-8", errors='replace'))
#                 max_calls = max(len(pid_dict['calls']) for pid_dict in cuckoo_report['behavior']['processes'])
#                 if limit_syscalls_length > max_calls:
#                     continue
#                 sha256 = cuckoo_report['sha256']
#                 syscalls_file_Path = cuckoo_reports_dir_Path / "flatten" / sha256
#                 if os.path.isfile(syscalls_file_Path):
#                     continue
#                 with open(file=syscalls_file_Path, mode='w', encoding="utf8") as syscalls_file:
#                      json.dump(cuckoo_report, syscalls_file)
#             except json.JSONDecodeError as err:
#                 print("process report {0}   JSONDecodeError error: {1}".format(sha256, err))
#                 pass
#             except:
#                 print("process_reports_file: {0}, error: {1}".format(file_name, sys.exc_info()))
#                 pass
#     except IOError as err:
#         print("process_reports_file   I/O error: {0}".format(err))
#     except:
#         print("process_reports_file: {0}, error: {1}".format(file_name, sys.exc_info()))


def process_reports_file(dict_manager, file_name):
    try:
        with open(file=cuckoo_reports_dir_Path/file_name, mode='rb') as file_handle:
                # 1. process report file to json
                cuckoo_report = json.load(file_handle)

                # 2. count syscalls from all processes:
                procs_list = cuckoo_report['behavior']['processes']
                calls_count = 0
                for proc_dict in procs_list:
                    calls_count += len(proc_dict["calls"])

                # 3. copy the reports which are above the minimum syscalls limit
                if minimum_syscalls_length <= calls_count:
                    # case of report which is above the minimum syscalls limit
                    ## sha256 = cuckoo_report['sha256']    # report doesnt contains sha256 - why???
                    dst_file_Path = output_dir_path_Path / file_name
                    if os.path.isfile(dst_file_Path):
                        dict_manager[file_name] = "dst file already exists."
                        return
                    with open(file=dst_file_Path, mode='w', encoding="utf8") as dst_file:
                         json.dump(cuckoo_report, dst_file)
                else:
                    # case of report which is below the minimum syscalls limit
                    return

    except json.JSONDecodeError as err:
        dict_manager[file_name] = "JSONDecodeError error."
        print("process report {0}   JSONDecodeError error: {1}".format(file_name, err))
        pass
    except:
        dict_manager[file_name] = "some error."
        print("process_reports_file: {0}, error: {1}".format(file_name, sys.exc_info()))
        pass


def main():
    if not os.path.exists(output_dir_path_Path):
        os.makedirs(output_dir_path_Path)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s",
        handlers=[
            logging.FileHandler(os.path.join(output_dir_path_Path, "log.txt")),
            logging.StreamHandler(sys.stdout)
        ])
    logger = logging.getLogger()
    logger.info("main func start, print arguments for this run:")


    # dict = {}
    # file_name = "eeda4f05ff83ee1dcfc86316e91d2ce69452e93bf1b1c604e50de891c21e87cc_report.json"
    # process_reports_file(dict, file_name)
    # print(dict)

    with multiprocessing.Manager() as manager:
        dict_manager = manager.dict()
        cpu_cores = 1
        if sys.platform == 'linux':
            # check avail cpu cores - only-Linux!
            cpu_cores = len(os.sched_getaffinity(0))
        else:
            # check CPU cores number - multi-platform
            cpu_cores = multiprocessing.cpu_count()
        starttime = time.time()
        num_processes = cpu_cores
        multiprocessing.freeze_support()
        with multiprocessing.Pool(num_processes) as pool:

            try:
                map_func = pool.apply_async
                a_results = [map_func(func=process_reports_file, args=(dict_manager, file_name))
                             for file_name in
                             itertools.islice(os.listdir(cuckoo_reports_dir_Path), 0, limit_reports_files)]
                pool.close()
                pool.join()
            except:
                logger.info("---- main loop error ----")
                logger.info(sys.exc_info)
                sys.exit(1)


        logger.info('That took {} seconds'.format(time.time() - starttime))
        dict_manager_file_path = os.path.join(output_dir_path_Path, "dict.json")
        with open(dict_manager_file_path, 'w') as output_file:
            json_dict = json.dumps(dict_manager.copy(), ensure_ascii=False)
            output_file.write(json_dict)  # .encode("utf-16"))

        logger.info("finished run, resulted dict in: " + dict_manager_file_path)

if __name__ == '__main__':
    try:
        main()
    except Exception as ex:
        print(ex)
        sys.exit(1)

