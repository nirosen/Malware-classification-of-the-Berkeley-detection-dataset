# import itertools
# import multiprocessing as mp
# import csv
# import sys
# import argparse
# import json
# import pickle
# import datetime
# import shutil
# #  from utils.logger import get_logger
# import datetime
# import multiprocessing
# import time
# import logging
# from pathlib import Path

import os
import glob
import json
import datetime
import pandas as pd
from collections import ChainMap

output_dir_path_Path = './merged/'

def main():
    #args = get_args()
    if not os.path.exists(output_dir_path_Path):
        os.makedirs(output_dir_path_Path)

    file_list = glob.glob("./output/*.json")
    allFilesDict = {v: k for v, k in enumerate(file_list, 1)}

    dicts_arr = []
    for k, v in allFilesDict.items():
        with open(v, 'r') as d:
            json_data = json.load(d)
            if json_data:
                dicts_arr.append(json_data)

    merged_dict = dict(ChainMap(*dicts_arr))

    output_merged_json = os.path.join(output_dir_path_Path,
                                      'merged' + str(len(file_list)) + '_' +
                                      datetime.datetime.now().strftime("%d%m%Y_%H%M%S") + '.json')
    with open(output_merged_json, 'w') as output_file:
        json_dict = json.dumps(merged_dict)
        output_file.write(json_dict)  # .encode("utf-16"))

    # df = pd.DataFrame(merged_dict)
    # df.to_json(os.path.join(output_dir_path_Path,
    #                         'merged' + str(len(file_list)) + '_' +
    #                         datetime.datetime.now().strftime("%d%m%Y_%H%M%S") + '.json'),
    #            orient='index')
    #
    # print('hi')


if __name__ == '__main__':
    try:
        main()
    except Exception as ex:
        print(ex)
        sys.exit(1)

