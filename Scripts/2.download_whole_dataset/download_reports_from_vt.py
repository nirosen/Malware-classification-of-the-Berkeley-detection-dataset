import itertools
import multiprocessing as mp
import csv
import sys
import argparse
import os
import json
import pickle
import datetime
import shutil
#  from utils.logger import get_logger
import datetime
import multiprocessing
import time
import logging
from pathlib import Path

import requests

PADDING = 0  # Placement for NOP

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
input_hashes_file = Path(PROJECT_ROOT, "miller_cuckoo_shuffle_70K.hashes")
apikey = "ac9ec065541fd8848e391bd3725854d8491cf5da4b7d523b623d4885e8e987ed"


output_dir_path_Path = Path(PROJECT_ROOT, "output")
# syscalls_dir_path_Path = Path(PROJECT_ROOT, "vt65k_12cores")
# vt_dir_path_Path = Path(PROJECT_ROOT, "vt65k_reports")
#
# limit_syscalls_files = None
# limit_syscalls_length = 500
limit_hashes = 19980

# def get_virustotal_result(report_path):
#     with open(report_path, 'r') as file_handle:
#         virustotal_report = json.load(file_handle)
#         score = float(virustotal_report["positives"]) / float(virustotal_report["total"])
#         score *= 10
#         # "first_seen": "2018-03-13 19:53:23"
#         first_seen_str = virustotal_report["first_seen"]
#         first_seen_obj = datetime.datetime.strptime(first_seen_str, '%Y-%m-%d %H:%M:%S')
#     return score, first_seen_obj


def download_vt_score_time(dict_manager, hash, logger):

    # VT API V2
    url = 'https://www.virustotal.com/vtapi/v2/file/report' # 'https://mail.google.com/mail/u/0/#inbox' #
    params = {'apikey': apikey, 'resource': hash, 'allinfo': 'true'}
    response = requests.get(url, params=params)

    if response.status_code == 200:
        try:
            response_dict = response.json()
            score = response_dict['positives'] / response_dict['total']
            first_submission_date_string = response_dict['first_seen'] # "2000-01-01 00:00:00"  first_seen":"2012-09-24 14:44:30"
            dict_manager[hash] = {
                "score": round(score, 3),
                "first_seen_month": datetime.datetime.strptime(
                    first_submission_date_string, '%Y-%m-%d %H:%M:%S').strftime("%Y-%m")
            }
        except IOError as err:
            print("download_vt_score_time   I/O error: {0}".format(err))
            logger.info("download_vt_score_time   I/O error: {0}".format(err))
        except:
            print("download_vt_score_time: {0}, some error: {1}".format(hash, sys.exc_info()))
            logger.info("download_vt_score_time: {0}, some error: {1}".format(hash, sys.exc_info()))

    #score = response_dict['positives'] / response_dict['total']
    #first_submission_date_string = response_dict['first_seen']
    #
    # try:
    #     dict_manager[hash] = {
    #         "score": str(score),
    #         "first_seen_obj": first_submission_date_string
    #     }





def main():
    #args = get_args()
    if not os.path.exists(output_dir_path_Path):
        os.makedirs(output_dir_path_Path)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s",
        handlers=[
            logging.FileHandler(os.path.join(output_dir_path_Path, "log.txt")),
            logging.StreamHandler(sys.stdout)
        ])
    logger = logging.getLogger()
    logger.info("\n\n\nmain func start, print arguments for this run:")
    #logger.info(args.__str__())


    print("start main")

    # with open(input_hashes_file) as hashes_file:
    #     hash = hashes_file.readline().rstrip('\n')
    #     print(hash)
    #     download_vt_score_time({}, hash, logger)

    #score, first_seen_obj = get_virustotal_result(input_hashes_file)
    #logger.info("file hash: {}, has the score: {}, and first_seen: {}".format(hash, str(score), first_submission_date_string))


    with multiprocessing.Manager() as manager:
        dict_manager = manager.dict()
        cpu_cores = 1
        if sys.platform == 'linux':
            # check avail cpu cores - only-Linux!
            cpu_cores = len(os.sched_getaffinity(0))
        else:
            # check CPU cores number - multi-platform
            cpu_cores = multiprocessing.cpu_count()
        starttime = time.time()
        num_processes = cpu_cores
        logger.info("cpu_cores: " + str(cpu_cores))
        multiprocessing.freeze_support()
        with multiprocessing.Pool(num_processes) as pool:

            try:
                hashes_list = [line.rstrip('\n') for line in open(input_hashes_file)]
                map_func = pool.apply_async
                [map_func(func=download_vt_score_time, args=(dict_manager, hash, logger))
                 for hash in
                 itertools.islice(hashes_list, 0, limit_hashes)]
                # output of func f is none
                # a_output = [p.get() for p in a_results]
                # print(a_output)
                pool.close()
                pool.join()
            except:
                logger.info("---- main loop error ----")
                logger.info(sys.exc_info)
                # output stdout to test.syscalls file
                # print(dict_manager.copy())
                sys.exit(1)
                # dict_manager_file_path = os.path.join(output_dir_path_Path, "errors.json")
                # with open(dict_manager_file_path, 'w') as output_file:
                #     json_dict = json.dumps(dict_manager.copy(), ensure_ascii=False)
                #     output_file.write(json_dict)  # .encode("utf-16"))

        logger.info('That took {} seconds'.format(time.time() - starttime))

        # output stdout to test.syscalls file
        # print(dict_manager.copy())
        dict_manager_file_path = os.path.join(output_dir_path_Path, "dict_score_time_" +
                                              datetime.datetime.now().strftime("%d%m%Y_%H%M%S") + ".json")
        with open(dict_manager_file_path, 'w') as output_file:
            json_dict = json.dumps(dict_manager.copy(), ensure_ascii=False)
            output_file.write(json_dict)  # .encode("utf-16"))

        # delete hashes from hashes_file:
        with open(input_hashes_file, 'r') as fin:
            data = fin.read().splitlines(True)
        with open(input_hashes_file, 'w') as fout:
            fout.writelines(data[limit_hashes:])

        logger.info("finished run, resulted dict in: " + dict_manager_file_path)

if __name__ == '__main__':
    try:
        main()
    except Exception as ex:
        print(ex)
        sys.exit(1)




# VIRUSTOTAL_AVS = {
#     "Bkav": 1.0,
#     "MicroWorld-eScan": 1.0,
#     "CMC": 1.0,
#     "CAT-QuickHeal": 1.0,
#     "McAfee": 1.0,
#     "Cylance": 1.0,
#     "Zillya": 1.0,
#     "AegisLab": 1.0,
#     "TheHacker": 1.0,
#     "K7GW": 1.0,
#     "K7AntiVirus": 1.0,
#     "TrendMicro": 1.0,
#     "Baidu": 1.0,
#     "NANO-Antivirus": 1.0,
#     "F-Prot": 1.0,
#     "Symantec": 1.0,
#     "TotalDefense": 1.0,
#     "TrendMicro-HouseCall": 1.0,
#     "Avast": 1.0,
#     "ClamAV": 1.0,
#     "GData": 1.0,
#     "Kaspersky": 1.0,
#     "BitDefender": 1.0,
#     "Babable": 1.0,
#     "ViRobot": 1.0,
#     "Rising": 1.0,
#     "Ad-Aware": 1.0,
#     "Sophos": 1.0,
#     "Comodo": 1.0,
#     "F-Secure": 1.0,
#     "DrWeb": 1.0,
#     "VIPRE": 1.0,
#     "Invincea": 1.0,
#     "McAfee-GW-Edition": 1.0,
#     "Emsisoft": 1.0,
#     "Ikarus": 1.0,
#     "Cyren": 1.0,
#     "Jiangmin": 1.0,
#     "Webroot": 1.0,
#     "Avira": 1.0,
#     "MAX": 1.0,
#     "Antiy-AVL": 1.0,
#     "Kingsoft": 1.0,
#     "Endgame": 1.0,
#     "Arcabit": 1.0,
#     "SUPERAntiSpyware": 1.0,
#     "ZoneAlarm": 1.0,
#     "Avast-Mobile": 1.0,
#     "Microsoft": 1.0,
#     "AhnLab-V3": 1.0,
#     "ALYac": 1.0,
#     "AVware": 1.0,
#     "TACHYON": 1.0,
#     "VBA32": 1.0,
#     "Malwarebytes": 1.0,
#     "Panda": 1.0,
#     "Zoner": 1.0,
#     "ESET-NOD32": 1.0,
#     "Tencent": 1.0,
#     "Yandex": 1.0,
#     "SentinelOne": 1.0,
#     "eGambit": 1.0,
#     "Fortinet": 1.0,
#     "AVG": 1.0,
#     "Cybereason": 1.0,
#     "Paloalto": 1.0,
#     "CrowdStrike": 1.0,
#     "Qihoo-360": 1.0
# }
