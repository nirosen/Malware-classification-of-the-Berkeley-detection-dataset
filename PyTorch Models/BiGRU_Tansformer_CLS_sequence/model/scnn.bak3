import torch.nn as nn
import torch.nn.utils as utils
import torch.nn.functional as F
import torch

# from .attention import SCNNAttention
from utils.logger import *


log = get_logger(__name__)



class SCNN(nn.Module):
    def __init__(self, device, batch_size, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(SCNN, self).__init__()
        self.device = device
        self.batch_size = batch_size

        self.embedding = nn.Embedding(num_embeddings=45, embedding_dim=128) # num_embeddings = Vocab-Size = 45
        self.gru = nn.GRU(input_size=128, hidden_size=64, num_layers=1, batch_first=True, dropout=0, bidirectional=True)
        self.normalize = nn.BatchNorm1d(num_features=64*2)
        self.linear = nn.Linear(64*2, 2) # bidirectional implies multiply 64 by 2

        # Drop some of the nodes to increase robustness in training
        self.dropout = nn.Dropout(0.5)

    def init_hidden(self):
        return torch.autograd.Variable(torch.randn(1*2, self.batch_size, 64)).cuda() # bidirectional implies (1*2, 10, 64)

    def forward(self, input_sequence, seq_lengths, hn=None):
        try:
            # input_sequence is already padded to MaxSeqLen = 1000
            embedded = self.embedding(input_sequence.long())#.permute(0, 2, 1)

            embedded_PackedSequence = utils.rnn.pack_padded_sequence(embedded.float(),
                                                                                  seq_lengths,
                                                                                  batch_first=True,
                                                                                  enforce_sorted=False)

            output, hn = self.gru(embedded_PackedSequence, hn)
            unpacked_output, unpacked_output_len = torch.nn.utils.rnn.pad_packed_sequence(output)
            unpacked_output = unpacked_output.permute(1,0,2)

            unpacked_output = self.normalize(unpacked_output[:,-1,:])

            # Perform final linear and softmax
            output = self.dropout(unpacked_output) # unpacked_output[:,-1,:].squeeze()) # output[:,-1,:].squeeze() => (10,1000,128) -> (10,128)
            output = F.log_softmax(self.linear(output)) # torch.cat dim=1

            return output, hn

        except Exception as ex:
            print(ex)
            print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)


            # output, hn = self.lstm(embedded)
            #normalized_output = self.normalize(output)
            #predicted_log_probabilities = F.log_softmax(self.linear(normalized_output), dim=1)
