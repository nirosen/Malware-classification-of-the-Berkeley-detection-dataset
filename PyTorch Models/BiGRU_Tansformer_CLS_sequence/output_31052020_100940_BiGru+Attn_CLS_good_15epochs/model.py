import torch.nn as nn
import torch.nn.utils as utils
import torch.nn.functional as F
import torch

# from .attention import SCNNAttention
from utils.logger import *


log = get_logger(__name__)


def create_sinusoidal_embeddings(embeds):
    position_enc = torch.tensor([
        [pos / np.power(10000, 2 * (j // 2) / embeds.embedding_dim) for j in range(embeds.embedding_dim)]
                                                                    for pos in range(embeds.num_embeddings)])
    embeds.weight[:, 0::2] = torch.sin(position_enc[:, 0::2])
    embeds.weight[:, 1::2] = torch.cos(position_enc[:, 1::2])
    embeds.weight.detach_()
    embeds.weight.requires_grad = False



class SCNN(nn.Module):
    def __init__(self, device, batch_size, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(SCNN, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.embed_size = 300#80
        self.hidden_size = 300#100
        self.gru_out_size = self.hidden_size*2 # biGru implies hidden_size*2
        self.attentions_out_size = self.gru_out_size
        self.MaxSeqLen = 10000

        self.causal = False

        self.tokens_embeddings = nn.Embedding(num_embeddings=45+1, embedding_dim=self.embed_size)

        # Notice: i'm not doing the position_embeddings on the sequence
        num_max_positions = self.MaxSeqLen # help: Max input length
        self.position_embeddings = nn.Embedding(num_embeddings=num_max_positions, embedding_dim=self.embed_size)

        sinusoidal_embeddings = False
        if sinusoidal_embeddings:
            create_sinusoidal_embeddings(self.position_embeddings)

        self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0.5, num_layers=1, bidirectional=True, batch_first=True)

        self.dropout = nn.Dropout(0.1)

        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()
        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()

        # Notive: my GPU 8GB memory can't handle more than 1 layer with self.embed_size=300
        num_layers=1
        for _ in range(num_layers):
            self.attentions.append(nn.MultiheadAttention(embed_dim=self.gru_out_size, num_heads=5, dropout=0.1))
            self.feed_forwards.append(nn.Sequential(nn.Linear(self.gru_out_size, self.hidden_size),
                                                    nn.ReLU(),
                                                    nn.Linear(self.hidden_size, self.gru_out_size)))
            self.layer_norms_1.append(nn.LayerNorm(self.gru_out_size, eps=1e-12))
            self.layer_norms_2.append(nn.LayerNorm(self.gru_out_size, eps=1e-12))

        self.linear = nn.Linear(self.attentions_out_size, 2)

    def init_hidden(self, batch_size, num_layers=1):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None, padding_mask=None):
        try:
            # padding each seq with zeros until max_seq_len = 10,000
            input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
            input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

            ### positions
            positions = torch.arange(len(input_sequence_padded), device=input_sequence_padded.device).unsqueeze(-1)

            ### tokens
            h = self.tokens_embeddings(input_sequence_padded)
            ## packing the seq destroyed my results...
            # input_sequence_packed = utils.rnn.pack_padded_sequence(input_sequence_padded,
            #                                                        seq_lengths,
            #                                                        batch_first=True,
            #                                                        enforce_sorted=False)
            # # Notice: i'm not doing the position_embeddings on the sequence
            # h = self.tokens_embeddings(input_sequence_packed.data)
            # h, _ = utils.rnn.pad_packed_sequence(utils.rnn.PackedSequence(h, input_sequence_packed.batch_sizes),
            #                                      batch_first=False)
            # h = h.permute(1, 0, 2)

            # add pos+tok
            h = h + self.position_embeddings(positions).expand_as(h)

            # run gru on packed embedded_seq (h)
            h_packed = utils.rnn.pack_padded_sequence(h, seq_lengths, batch_first=True, enforce_sorted=False)
            hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
            self.gru.flatten_parameters()
            gru_out, _ = self.gru(h_packed, hidden)
            gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)
            gru_out = gru_out.permute(1, 0, 2)

            h = gru_out

            h = self.dropout(h)

            # set x to unpacked input
            x = input_sequence_padded

            attn_mask = None
            if self.causal:
                attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)
                attn_mask = torch.triu(attn_mask, diagonal=1)

            for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,
                                                                           self.layer_norms_2, self.feed_forwards):
                h = layer_norm_1(h)
                x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)
                x = self.dropout(x)
                h = x + h

                h = layer_norm_2(h)
                x = feed_forward(h)
                x = self.dropout(x)
                h = x + h

            transformer_out = torch.transpose(h, 1, 2).contiguous()

            #pooled = F.max_pool1d(h_out, h_out.size(2)).squeeze(2)
            #relued = F.relu(pooled)
            #out = F.dropout(relued, p=0.1)

            #out = F.log_softmax(self.linear(transformer_out.mean(dim=2)))
            out = F.log_softmax(self.linear(transformer_out[:,:,0]))

            torch.cuda.empty_cache()  # clear GPU memory

            return out

        except Exception as ex:
            print(ex)
            print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)