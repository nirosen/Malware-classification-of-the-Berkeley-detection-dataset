import torch.nn as nn
import torch.nn.utils as utils
import torch.nn.functional as F
import torch

# from .attention import SCNNAttention
from utils.logger import *


log = get_logger(__name__)


class SCNN(nn.Module):
    def __init__(self, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(SCNN, self).__init__()
        # self.log = log
        #
        # self.categories_count = categories_count
        # self.embedding_size = embedding_size
        # self.hidden_size = hidden_size
        # self.classes_count = classes_count
        #
        # self.embedding = nn.Embedding(categories_count, embedding_size, 0)
        # self.gru = nn.GRU(embedding_size, hidden_size)  # Dan also added MultiHeadAtt
        # self.normalize = nn.BatchNorm1d(hidden_size)
        # self.linear = nn.Linear(hidden_size, classes_count)
        #
        self.retry = 3

        #self.linear2 = nn.Linear(5922, 2) # TODO: make TFIDF vector size generic!

        # Create word embeddings from the input words
        self.embedding = nn.Embedding(45, 100, 0)

        # Specify convolutions with filters of different sizes (fs)
        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1,
                                              out_channels=100,
                                              kernel_size=(fs, 100))
                                    for fs in [1,2,3,4,5]])

        # Add a fully connected layer for final predicitons
        self.linear = nn.Linear(len([1,2,3,4,5])  * 100, 2)

        # Drop some of the nodes to increase robustness in training
        self.dropout = nn.Dropout(0.5)

    def forward(self, input_sequence):
        for try_count in range(self.retry):
            try:
                # embedding = self.embedding(input_sequence.data.long())
                # #embedding = self.embedding(input_sequence.data)
                # embedded = utils.rnn.PackedSequence(embedding, input_sequence.batch_sizes)
                # output, hn = self.gru(embedded)
                # output = hn.squeeze()
                # predicted_log_probabilities = F.log_softmax(self.linear(output), dim=1)

                #embedding = self.embedding(input_sequence.data.long())
                #embedding_new = self.embedding_new(input_sequence.data.long())
                #ret_new = self.fc_new(embedding_new)

                #predicted_log_probabilities2 = self.linear2(input_sequence.data.float())
                #predicted_log_probabilities2_softmax = F.log_softmax(predicted_log_probabilities2, dim=1)
                ##predicted_log_probabilities2_logsigmoid = F.logsigmoid(predicted_log_probabilities2)
                #return predicted_log_probabilities2_softmax

                # Get word embeddings and formt them for convolutions
                embedded = self.embedding(input_sequence.long()).unsqueeze(1)

                # Perform convolutions and apply activation functions
                conved = [F.relu(conv(embedded)).squeeze(3)
                          for conv in self.convs]

                # Pooling layer to reduce dimensionality
                pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2)
                          for conv in conved]

                # Dropout layer
                cat = self.dropout(torch.cat(pooled, dim=1))

                return F.log_softmax(self.linear(cat), dim=1)

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)
                continue


                # output, hn = self.lstm(embedded)
                #normalized_output = self.normalize(output)
                #predicted_log_probabilities = F.log_softmax(self.linear(normalized_output), dim=1)
