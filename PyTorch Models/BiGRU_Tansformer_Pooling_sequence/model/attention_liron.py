import numpy as np
import torch
from torch import nn
from torch.nn import functional as F
from torch.nn.utils import rnn as rnn_utils
from warnings import warn

num_categories = 6  # different syscall types
max_batch_seq_len = 40  # The maximal length after padding or cutting the seq
batch_size = 3

embedding_dim = 8
rnn_output_dim = 7
num_classes = 2
hidden_dim = 20
num_epochs = 2

# Param for generating the synthetic data
min_seq_len_gen = 5
max_seq_len_gen = 50
num_data_points = 13

# Generate the synthetic data
seq_lens = np.random.randint(min_seq_len_gen, max_seq_len_gen + 1, num_data_points)
raw_seq = [np.random.randint(0, num_categories, size=seq_len) for seq_len in seq_lens]
raw_seq = sorted(raw_seq, key=len, reverse=True)
seq_lens = [len(s) for s in raw_seq]
labels = np.random.randint(0, 2, num_data_points)

def cut_seq(seq, max_len, selection_method='random', dtype=np.int64):
    if len(seq) <= max_len:
        out = seq
    elif selection_method == 'start':
        out = seq[:max_len]
    elif selection_method == 'end':
        out = seq[-max_len:]
    else:
        if not selection_method == 'random':
            warn("Unknown selection method, defaulting to random")
        start_ind = np.random.randint(0, len(seq) - max_len)
        out = seq[start_ind:start_ind+max_len]
    return out.astype(dtype)


def batch_generator(seq_list, labels, max_seq_len, batch_size=5, cut_selection_method='random', max_epochs=np.inf, max_batchs=np.inf, sample_inds=None):
    batchs_yielded = 0
    epochs_loaded = 0
    if not sample_inds:
        sample_inds = np.array([]).astype(np.int32)
    while batchs_yielded < max_batchs and epochs_loaded < max_epochs + 1:
        while len(sample_inds) < batch_size:
            new_inds = np.arange(len(seq_list))
            np.random.shuffle(new_inds)
            sample_inds = np.concatenate((sample_inds, new_inds))
            epochs_loaded += 1
        batchs_yielded += 1
        cur_samp_ind = sorted(sample_inds[:batch_size])
        yield epochs_loaded, batchs_yielded, [cut_seq(seq_list[i],max_seq_len, cut_selection_method) for i in cur_samp_ind], labels[cur_samp_ind]
        sample_inds = sample_inds[batch_size:]


def sort_by_len(seqs):
    seqs = sorted(seqs, key=len, reverse=True)
    seq_lens = [len(s) for s in seqs]
    return seqs, seq_lens


def padded_to_seq_list(padded, seq_lens):
    return [padded[:l,i,:] for i,l in enumerate(seq_lens)]


emb = nn.Embedding(num_categories, embedding_dim, 0)
gru = nn.GRU(embedding_dim, hidden_dim)

#%%

class Attention(nn.Module):
    def __init__(self, input_dim, num_attn_heads, attn_dim, max_len, mask_val=-1e9):
        super().__init__()
        if not attn_dim * num_attn_heads == input_dim:
            raise ValueError(f"The attention input dim {input_dim} isn't "
                             f"equal to the dimension of an attention head "
                             f"{attn_dim} times the number of attention heads "
                             f"{num_attn_heads}")

        self.input_dim = input_dim
        self.num_attn_heads = num_attn_heads
        self.attn_dim = attn_dim
        
        self.k_linear = nn.Linear(attn_dim, attn_dim)
        self.v_linear = nn.Linear(attn_dim, attn_dim)
        self.out_linear = nn.Linear(input_dim, input_dim)
        
        self.max_len = max_len
        pe_base = torch.zeros(max_len, input_dim).float()
        pe_base.require_grad = False
        position = torch.arange(0, max_len).float().unsqueeze(1)
        pe_div_term = (torch.arange(0, input_dim, 2).float() * -(np.log(1e4) / input_dim)).exp()
        pe_base[:, 0::2] = torch.sin(position * pe_div_term)
        pe_base[:, 1::2] = torch.cos(position * pe_div_term)
        self.pe_base = pe_base
        
        self.attn_score_multiplier =  1 / (attn_dim**0.5)
        self.mask_val = mask_val
        return

    def with_pe(self, x, batch_max_len, batch_size):
        pe = torch.unsqueeze(self.pe_base[:batch_max_len,:],1).expand(\
                            -1, batch_size, -1)
        return x + pe
    
    def get_len_mask(self, lens, mask_dtype=torch.uint8):
        len_mask = torch.zeros((len(lens), max(lens)), dtype=mask_dtype)
        for i, cur_seq_len in enumerate(lens):
            len_mask[i, cur_seq_len:] = 1
        return len_mask.unsqueeze(1).unsqueeze(2)
        
    def apply_attention(self, x, lens):
        if max(lens) > self.max_len:
            raise ValueError("The current sequence is longer than the "
                             f"maximal sequence length: {max(lens)} > "
                             f"{self.max_len}")
        batch_max_len, batch_size = x.size()[:2]
        x = self.with_pe(x, batch_max_len, batch_size)
                
        x_attn_reshaped = x.view(batch_size, -1, self.num_attn_heads, \
                                 self.attn_dim).transpose(1, 2)
        k = self.k_linear(x_attn_reshaped).squeeze(-1)
        v = self.v_linear(x_attn_reshaped)
        
        attn_scores = k.transpose(-2, -1).masked_fill(self.get_len_mask(lens), self.mask_val)
        normlized_weights = F.softmax(attn_scores, dim=-1)
        attended = torch.matmul(normlized_weights, v).squeeze(1)
        attended = attended.transpose(1, 2).contiguous().view(-1, self.attn_dim * self.num_attn_heads)
        return attended
    
    def forward(self, x, lens):
        attended = self.apply_attention(x,lens)
        return attended

attention = Attention(hidden_dim, num_attn_heads=5, attn_dim=4, max_len=max_seq_len_gen)

#%%
packed_input = rnn_utils.pack_sequence([torch.from_numpy(s.astype(np.int64)) for s in raw_seq])
embedded = rnn_utils.PackedSequence(emb.forward(packed_input.data), packed_input.batch_sizes)
predicted = gru.forward(embedded)[0]  # [0] selects the sequence of outputs, not just the output from the last step
padded_predicted = rnn_utils.pad_packed_sequence(predicted)
attended = attention.forward(*padded_predicted)

