import os

import tqdm
import torch.nn as nn

from utils.logger import *
from utils.constants import *
from dataset.utils import *

import numpy as np


class SCNNTrainer:
    """

    """
    def __init__(self, arguments, scnn, device, learning_rate, train_dataloader, validation_dataloader, test_dataloader):
        """
        docstring here
            :param self: 
            :param arguments: 
            :param scnn: 
            :param device: 
            :param learning_rate: 
            :param train_dataloader: 
            :param validation_dataloader: 
            :param test_dataloader: 
        """   # arguments initialization
        self.device = device
        self.scnn = scnn
        self.batch_size = arguments.batch_size
        self.batch_iterations = arguments.batch_count
        self.learning_rate = learning_rate
        self.train_dataloader = train_dataloader
        self.train_dataloader = train_dataloader
        self.validation_dataloader = validation_dataloader
        self.test_dataloader = test_dataloader
        self.log = get_logger(__name__)
        self.output_dir = arguments.output_dir

        # properties initialization
        self.scnn = self.scnn.to(self.device)

        self.optimizer = torch.optim.Adam(self.scnn.parameters(), lr=self.learning_rate, weight_decay=0.00000001)
        # load trained optimizer
        if arguments.load_model_file != 'none':
            self.log.info(f"Loading pre-trained optimizer from {arguments.load_model_file}")
            checkpoint = torch.load(arguments.load_model_file)
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        self.criterion = nn.NLLLoss().to(self.device)
        #self.criterion = nn.CrossEntropyLoss().to(self.device)
        #self.criterion = nn.BCEWithLogitsLoss().to(self.device)

        # clear GPU memory
        torch.cuda.empty_cache()

    def train(self, epoch):
        return self.__model_iteration(epoch, self.train_dataloader, TRAIN)

    def validation(self, epoch):
        return self.__model_iteration(epoch, self.validation_dataloader, VALIDATION)

    def test(self):
        return self.__model_iteration(0, self.test_dataloader, TEST)

    def save(self, epoch, loss, output_dir):
        saved_model_dir = os.path.join(output_dir, 'saved_model')
        if not os.path.exists(saved_model_dir):
            os.mkdir(saved_model_dir)
        output_path = os.path.join(saved_model_dir, f"epoch{epoch}"+'.trained_model')
        torch.save(obj={
            #'model_cpu': self.scnn.cpu(),
            'epoch': epoch,
            'model_state_dict': self.scnn.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': loss}, f=output_path, _use_new_zipfile_serialization=False) # TODO: arg True for pytorch>=1.6 )
        #self.scnn.to(self.device)
        self.log.info(f"Epoch {epoch} - Saved Model: {output_path}")
        return output_path

    def show_metrics(self, y_true, y_predicted):
        # True positive
        tp = np.sum(y_true * y_predicted)
        # False positive
        fp = np.sum((y_true == 0) * y_predicted)
        # True negative
        tn = np.sum((y_true==0) * (y_predicted==0))
        # False negative
        fn = np.sum(y_true * (y_predicted==0))

        # True positive rate (sensitivity or recall)
        tpr = tp / (tp + fn)
        # False positive rate (fall-out)
        fpr = fp / (fp + tn)
        # Precision
        precision = tp / (tp + fp)
        # True negatvie tate (specificity)
        tnr = 1 - fpr
        # F1 score
        f1 = 2*tp / (2*tp + fp + fn)
        # ROC-AUC for binary classification
        auc = (tpr+tnr) / 2
        # MCC
        mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))

        print("True positive: ", tp)
        print("False positive: ", fp)
        print("True negative: ", tn)
        print("False negative: ", fn)

        print("True positive rate (recall): ", tpr)
        print("False positive rate: ", fpr)
        print("Precision: ", precision)
        print("True negative rate: ", tnr)
        print("F1: ", f1)
        print("ROC-AUC: ", auc)
        print("MCC: ", mcc)

    def __model_iteration(self, epoch, data_loader, type):
        tqdm_data_iteration = tqdm.tqdm(iterable=enumerate(data_loader),
                                        desc=f"Epoch {type} - {epoch}",
                                        total=len(data_loader),
                                        bar_format="{l_bar}{bar}{r_bar}")
        
        # set whether we are in train iteration or not
        train = True if type is TRAIN else False

        # initialize iteration variables
        avg_loss = 0.0
        sum_loss = 0.0
        total_true_positive = 0
        total_prediction = 0
        iteration = 0
        sum_accuracy = 0.0
        sum_auc = 0.0
        ret = {}

        actuals0 = []
        actuals1 = []
        probabilities0 = []
        probabilities1 = []
        predictions = []
        which_class = 0

        # sum batches for epoch metrics
        test_probs = 0
        testy = 0
        testy_pred = 0

        # initialize the optimizer gradients to zero
        self.optimizer.zero_grad()
        #torch.backends.cudnn.enabled = False

        #hn = self.scnn.init_hidden()

        # iterate the data_loader for batches, data is received as tensors
        for data in data_loader:
            if iteration == 20:
                a = 'break'
            # increase iteration
            iteration += 1

            if len(data["label"]) !=self.batch_size:
                continue

            # batch data sent into the device (GPU or CPU)
            # NIR: error in dict-to-gpu , changed below with stack()
            data["sequence"] = torch.stack(data["sequence"]).transpose(0,1)
            #data["sequence"] = data["sequence"].to(self.device)
            #data["label"] = data["label"].to(self.device)
            data = {key: value.to(self.device) for key, value in data.items()}


            # set sequence and label variables
            data_sequence = data["sequence"]
            data_label = data["label"]
            seq_lengths = data["seq_length"]

            # create the packed sequence from the batch tensors for the model
            #packed_data_sequence = pack_sequence_tensor(data_sequence)

            # forward the packed sequences batch and get the predicted log probabilities
            #predicted_log_probs = self.scnn(packed_data_sequence)
            predicted_log_probs = self.scnn(data_sequence, seq_lengths) #, hn)
            if torch.isnan(predicted_log_probs).any():
                a = 'break'

            # TODO: nir found exception in row pred_loss: Expected input batch_size (10000) to match target batch_size (10).
            # NLL(negative log likelihood) loss of the log probs classification result
            #pred_loss = self.criterion(predicted_log_probs[:,0], data_label.float())  # <-- for BCE loss
            try:
                pred_loss = self.criterion(predicted_log_probs, data_label)  # <-- for NLL loss
            except Exception as ex:
                a = 'break'

            #lmbda = 0.000000001
            #for name, param in self.scnn.named_parameters():
            #    if 'bias' not in name:
            #        pred_loss += lmbda * param.pow(2).sum()

            #for p in self.scnn.parameters():
            #    pred_loss += lmbda * p.pow(2).sum()

            # 3. backward and optimization only in train
            if train:
                self.optimizer.zero_grad()
                pred_loss.backward(retain_graph=True) # added retain_graph=True after BiGru
                self.optimizer.step()
                # TODO: update learning rate ?
            # 3.2 in case of valid/test epoch - calc evaluation metrics per batch
            else:
                # TODO = check this!
                prediction = predicted_log_probs.argmax(dim=1, keepdim=True)
                predictions.extend(prediction.to('cpu').detach().numpy().flatten().astype(int))
                actuals0.extend(data_label.view_as(prediction == 0).to('cpu').detach().numpy().flatten().astype(int))
                actuals1.extend(data_label.view_as(prediction == 1).to('cpu').detach().numpy().flatten().astype(int))


                probabilities0.extend(torch.exp(predicted_log_probs).to('cpu').detach().numpy()[:, 0])
                probabilities1.extend(torch.exp(predicted_log_probs).to('cpu').detach().numpy()[:, 1])
                if np.isnan(probabilities0).any() or np.isnan(probabilities1).any():
                    a = 'break'

                # calc eptch acc
                predicted_probs = torch.exp(predicted_log_probs)
                top_p, top_class = predicted_probs.topk(1, dim=1)
                equals = top_class == data_label.view(*top_class.shape)
                sum_accuracy += torch.mean(equals.type(torch.FloatTensor))

                # calc per-batch mertics
                y_true = data_label.to('cpu').numpy()
                y_predicted = predicted_log_probs.argmax(dim=1).to('cpu').numpy()
                # True positive
                tp = np.sum(y_true * y_predicted)
                # False positive
                fp = np.sum((y_true == 0) * y_predicted)
                # True negative
                tn = np.sum((y_true == 0) * (y_predicted == 0))
                # False negative
                fn = np.sum(y_true * (y_predicted == 0))
                # True positive rate (sensitivity or recall)
                tpr = tp / (tp + fn)
                # False positive rate (fall-out)
                fpr = fp / (fp + tn)
                # Precision
                precision = tp / (tp + fp)
                # True negatvie tate (specificity)
                tnr = 1 - fpr
                # F1 score
                #f1 = 2 * tp / (2 * tp + fp + fn)
                # ROC-AUC for binary classification
                auc = (tpr + tnr) / 2
                # MCC
                #mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))

                sum_auc += auc



            # 4. check sequence prediction accuracy
            correct = predicted_log_probs.argmax(dim=1).eq(data_label).sum().item()
            sum_loss += pred_loss.item()
            avg_loss = sum_loss / iteration
            total_true_positive += correct
            total_prediction += predicted_log_probs.shape[0]

            tqdm_data_iteration.update()
            if iteration % 10 == 0:
                tqdm_post = {
                    "epoch": epoch,
                    "iter": iteration,
                    "avg_loss": avg_loss,
                    "avg_acc": total_true_positive / total_prediction * 100,
                    "loss": pred_loss.item()
                }
                tqdm_data_iteration.write(str(tqdm_post))
                self.log.debug(str(tqdm_post))

            # 5. check prediction metrics

            # if data['label'].shape[0] != 10:
            #     a = 'break'
            # self.show_metrics(y_true=data_label.to('cpu').numpy(), y_predicted=predicted_log_probs.argmax(dim=1).to('cpu').numpy())
            #print('break')

            torch.cuda.empty_cache() # clear GPU memory


        self.log.info(f"Epoch {type} - {epoch}: average_loss={avg_loss}, iteration_accuracy={total_true_positive * 100.0 / total_prediction}")
        # summed-loss of the batches, normalized by number of batches
        loss_normalized = (sum_loss/len(data_loader))
        if train:
            return {'loss': loss_normalized}
        else:
            # plot_curves for which_class=0
            title = type + ' epoch-' + str(epoch) + '-class-0'
            metrics0 = self.epoch_metrics(title=title, test_probs=probabilities0, testy=actuals0, testy_pred=predictions, which_class=0)

            # plot_curves for which_class=1
            title = type + ' epoch-' + str(epoch) + '-class-1'
            metrics1 = self.epoch_metrics(title=title, test_probs=probabilities1, testy=actuals1, testy_pred=predictions, which_class=1)

            # return metrics for simple graphs - TODO - fix to dup which_class !!
            accuracy_normalized = (sum_accuracy/len(data_loader))
            auc_normalized = (sum_auc/len(data_loader))
            #return {'loss': loss_normalized, 'accuracy': accuracy_normalized, 'auc': auc_normalized}
            return {'loss': loss_normalized, 'accuracy': accuracy_normalized, 'auc': metrics1['ROC_auc'], 'metrics0': metrics0, 'metrics1': metrics1}


    def epoch_metrics(self, title, test_probs, testy, testy_pred, which_class):
        # fit the training dataset on the classifier
        #model.fit(trainX, trainy)
        # predict probabilities
        #model_probs = model.predict_proba(testX)
        # keep probabilities for the positive outcome only
        #model_probs = model_probs[:, 1] -> test_probs
        # predict class values
        #testy_pred = model.predict(testX) -->
        #import sklearn
        #import sklrearn.metrics

        from sklearn.metrics import precision_recall_curve, f1_score, auc, accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve
        import matplotlib.pyplot as plt

        model_precision_class, model_recall_class, _ = precision_recall_curve(testy, test_probs, pos_label=which_class)
        model_PR_f1_class, model_PR_auc = f1_score(testy, testy_pred, pos_label=which_class), auc(model_recall_class, model_precision_class)
    
        # nir added metricsPR-
        acc = accuracy_score(testy, testy_pred)
        precision_class, recall_class, ROC_auc = precision_score(testy, testy_pred, pos_label=which_class), recall_score(testy, testy_pred, pos_label=which_class), roc_auc_score(testy, test_probs)
        # summarize scores
        print('\t Model: precision=%.3f recall=%.3f ROC-auc=%.3f accuracy=%.3f P@R-f1=%.3f P@R-auc=%.3f' % (
        precision_class, recall_class, ROC_auc, acc, model_PR_f1_class, model_PR_auc))
    
        # create 2 plots:
        _, axes = plt.subplots(1, 2, figsize=(20, 5))
        # plot the precision-recall curves
        no_skill = len(np.where(np.array(testy) == which_class)[0]) / len(testy)
        #no_skill = len(testy[testy == 1]) / len(testy)
        axes[0].plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')
        axes[0].plot(model_recall_class, model_precision_class, marker='.', label='Model')
        # set title
        axes[0].set_title('Precision@Recall-Curve\n' + title)
        # axis labels
        axes[0].set_xlabel('Recall')
        axes[0].set_ylabel('Precision')
        # show the legend
        axes[0].legend()
        # generate a no skill prediction (majority class)
        ns_probs = [0 for _ in range(len(testy))]
        # calculate scores
        ns_auc = roc_auc_score(testy, ns_probs)
        model_auc = roc_auc_score(testy, test_probs)
        # summarize scores
        print('\t No Skill: ROC AUC=%.3f' % (ns_auc))
        print('\t Model: ROC AUC=%.3f' % (model_auc))
        # calculate roc curves
        ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs, pos_label=which_class)
        model_fpr, model_tpr, _ = roc_curve(testy, test_probs, pos_label=which_class)
        # plot the roc curve for the model
        axes[1].plot(ns_fpr, ns_tpr, linestyle='--', label=('No Skill AUC=%.3f' % (ns_auc)))
        axes[1].plot(model_fpr, model_tpr, marker='.', label=(f'Model AUC=%.3f' % (model_auc)))
        # set title
        axes[1].set_title('ROC-curve\n' + title)
        # axis labels
        axes[1].set_xlabel('False Positive Rate')
        axes[1].set_ylabel('True Positive Rate')
        # show the legend
        axes[1].legend()
        plt.savefig(os.path.join(self.output_dir, ('curves_' + title.replace('\n', '_').replace(' ', '-') + ".png")))
        #plt.show()

        ret = {}
        #ret['model'] = model_name
        ret['precision_class'] = precision_class
        ret['recall_class'] = recall_class
        ret['ROC_auc'] = ROC_auc
        ret['acc'] = acc
        ret['model_PR_f1_class'] = model_PR_f1_class
        ret['model_PR_auc'] = model_PR_auc
        ret['ns_auc'] = ns_auc
        ret['model_auc'] = model_auc
        ret['model_fpr'] = model_fpr
        ret['model_tpr'] = model_tpr

        return ret