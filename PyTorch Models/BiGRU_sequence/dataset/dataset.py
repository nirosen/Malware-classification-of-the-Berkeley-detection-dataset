import json
from pathlib import Path
import os
import numpy as np

from torch.utils.data import Dataset
from utils.logger import *
from utils.constants import *

#from torch import FloatTensor
import torch
import numpy

log = get_logger(__name__)

# nir imports for cross-processes-syscalls-sorted-by-time
import itertools
from datetime import datetime


# nir imports for train balanced batch csv
import pandas as pd


class CuckooSCNNCSVWordsDataset(Dataset):
    def __init__(self, arguments, ds_type, on_memory=False):
        self.log = get_logger(__name__)
        self.data_path = os.path.join(arguments.dataset_path, ds_type)
        self.min_length = arguments.min_sequence_length
        self.max_length = arguments.max_sequence_length
        self.on_memory = on_memory
        self.max_syscalls_types = arguments.categories_input_layer
        self.balaced_order_csv_path = os.path.join(arguments.csv_dir, (ds_type+"_balaced_order.csv"))


        # use csv for balanced batch only in the train dataset:
        # if ds_type == TRAIN:
        #     self.train_df = pd.read_csv(self.csv_path)
        #     self.sequences = [os.path.join(self.data_path, row['hash']) for index, row in self.train_df.iterrows()
        #                       if os.path.isfile(os.path.join(self.data_path, row['hash']))]
        #     self.sequences_count = len(self.sequences)
        # else:
        #     # NIR: fixed name -> full path
        #     # self.sequences = [name for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]
        #     self.sequences = [os.path.join(self.data_path, name) for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]
        #     self.sequences_count = len(self.sequences)
        self.balaced_order_df = pd.read_csv(self.balaced_order_csv_path)
        self.sequences = [os.path.join(self.data_path, row['hash']) for _, row in self.balaced_order_df.iterrows()
                          if os.path.isfile(os.path.join(self.data_path, row['hash']))]
        self.sequences_count = len(self.sequences)
        self.scores = [row['score'] for _, row in self.balaced_order_df.iterrows()]


        self.syscall_mapping = {}
        self.syscall_counter = 0

        self.log.info(f"Initialized CuckooSCNNDataset instance object for dataset type: {ds_type} of size: {self.sequences_count}.")

    def __len__(self):
        return self.sequences_count

    def __getitem__(self, item):
        return None if self.on_memory else self.__load_cuckoo_syscalls_sequence(item, self.sequences[item], self.scores[item])

    def __load_cuckoo_syscalls_sequence(self, item, sequence_file_path, score):
        syscalls_sequence = []
        syscalls_label = 0

        # score = 0
        # labels_dict_Path = Path(sequence_file_path).parent.parent / "labels.json"
        # with open(labels_dict_Path, 'rb') as file_handle:
        #     labels_dict = json.load(file_handle)
        #     score = numpy.float64(labels_dict[Path(sequence_file_path).name]['score'])

        score = self.scores[item]
        syscalls_label = 1 if score > 5 else 0

        with open(sequence_file_path, 'rb') as file_handle:
            try:
                # 1. process report file to json
                cuckoo_report = json.load(file_handle)

                # 2. extract sorted calls by time:
                procs_list = cuckoo_report['behavior']['processes']
                extracted_procs_dict_list = []
                for proc_dict in procs_list:
                    calls_list = proc_dict["calls"]
                    extracted_proc_dict_list = []
                    for call_dict in itertools.islice(calls_list, 0, self.max_length):
                        timestamp_str = call_dict['timestamp']
                        # if to syscall-timestamp is not valid, skip this syscall to the next one.
                        try:
                            date_time_obj = datetime.strptime(timestamp_str, '%Y%m%d%H%M%S.%f')
                        except:
                            continue
                        syscall_str = call_dict['api']
                        extracted_calls_times_dict = {}
                        extracted_calls_times_dict['date_time_obj'] = date_time_obj
                        extracted_calls_times_dict['syscall_str'] = syscall_str
                        extracted_proc_dict_list.append(extracted_calls_times_dict)
                    extracted_procs_dict_list.extend(extracted_proc_dict_list)
                extracted_procs_dict_list = sorted(extracted_procs_dict_list,
                                                   key=lambda call: call['date_time_obj'],
                                                   reverse=False)[0:self.max_length]



                # 3. extracted_procs_dict_list -> to list of words
                for syscall in (d['syscall_str'] for d in extracted_procs_dict_list):
                    syscalls_sequence.append(syscall)

            except json.JSONDecodeError as err:
                print("process report {0}   JSONDecodeError error: {1}".format(report_file, err))
                pass
            except:
                print("process_reports_file: {0}, error: {1}".format(report_file, sys.exc_info()))
                pass

        syscalls_data = {
            "index": item,
            "sequence": syscalls_sequence, #np.array(syscalls_sequence, dtype=np.int64), #torch.Tensor(syscalls_sequence),
            "label": syscalls_label #numpy.int64(syscalls_label) # not working! int and to list..
        }
        return syscalls_data


class CuckooSCNNCSVVectorizerDataset(Dataset):
    def __init__(self, arguments, ds_type, on_memory=False):
        self.log = get_logger(__name__)
        self.data_path = os.path.join(arguments.dataset_path, ds_type)
        self.min_length = arguments.min_sequence_length
        self.max_length = arguments.max_sequence_length
        self.on_memory = on_memory
        self.max_syscalls_types = arguments.categories_input_layer
        self.balaced_order_csv_path = os.path.join(arguments.csv_dir, (ds_type+"_balaced_order.csv"))
        import pickle
        self.fitted_vectorizer = pickle.load(open(os.path.join(arguments.csv_dir, "fitted_vectorizer_1-1.pickle"), "rb"))


        # use csv for balanced batch only in the train dataset:
        # if ds_type == TRAIN:
        #     self.train_df = pd.read_csv(self.csv_path)
        #     self.sequences = [os.path.join(self.data_path, row['hash']) for index, row in self.train_df.iterrows()
        #                       if os.path.isfile(os.path.join(self.data_path, row['hash']))]
        #     self.sequences_count = len(self.sequences)
        # else:
        #     # NIR: fixed name -> full path
        #     # self.sequences = [name for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]
        #     self.sequences = [os.path.join(self.data_path, name) for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]
        #     self.sequences_count = len(self.sequences)
        self.balaced_order_df = pd.read_csv(self.balaced_order_csv_path)
        self.sequences = [os.path.join(self.data_path, row['hash']) for _, row in self.balaced_order_df.iterrows()
                          if os.path.isfile(os.path.join(self.data_path, row['hash']))]
        self.sequences_count = len(self.sequences)
        self.scores = [row['score'] for _, row in self.balaced_order_df.iterrows()]


        self.syscall_mapping = {}
        self.syscall_counter = 0

        self.log.info(f"Initialized CuckooSCNNDataset instance object for dataset type: {ds_type} of size: {self.sequences_count}.")

    def __len__(self):
        return self.sequences_count

    def __getitem__(self, item):
        return None if self.on_memory else self.__load_cuckoo_syscalls_sequence(item, self.sequences[item], self.scores[item])

    def __load_cuckoo_syscalls_sequence(self, item, sequence_file_path, score):
        #syscalls_sequence = []
        #syscalls_label = 0

        # score = 0
        # labels_dict_Path = Path(sequence_file_path).parent.parent / "labels.json"
        # with open(labels_dict_Path, 'rb') as file_handle:
        #     labels_dict = json.load(file_handle)
        #     score = numpy.float64(labels_dict[Path(sequence_file_path).name]['score'])

        score = self.scores[item]
        syscalls_label = 1 if score > 5 else 0

        seq_sentence = ''
        seq_sentence_vectorized = 0

        with open(sequence_file_path, 'rb') as file_handle:
            try:
                # 1. process report file to json
                cuckoo_report = json.load(file_handle)

                # 2. extract sorted calls by time:
                procs_list = cuckoo_report['behavior']['processes']
                extracted_procs_dict_list = []
                for proc_dict in procs_list:
                    calls_list = proc_dict["calls"]
                    extracted_proc_dict_list = []
                    for call_dict in itertools.islice(calls_list, 0, self.max_length):
                        timestamp_str = call_dict['timestamp']
                        # if to syscall-timestamp is not valid, skip this syscall to the next one.
                        try:
                            date_time_obj = datetime.strptime(timestamp_str, '%Y%m%d%H%M%S.%f')
                        except:
                            continue
                        syscall_str = call_dict['api']
                        extracted_calls_times_dict = {}
                        extracted_calls_times_dict['date_time_obj'] = date_time_obj
                        extracted_calls_times_dict['syscall_str'] = syscall_str
                        extracted_proc_dict_list.append(extracted_calls_times_dict)
                    extracted_procs_dict_list.extend(extracted_proc_dict_list)
                extracted_procs_dict_list = sorted(extracted_procs_dict_list,
                                                   key=lambda call: call['date_time_obj'],
                                                   reverse=False)[0:self.max_length]

                seq_sentence = ' '.join(d['syscall_str'] for d in extracted_procs_dict_list)
                #syscalls_sequence.append(seq_sentence)
                seq_sentence_vectorized = self.fitted_vectorizer.transform([seq_sentence])
                seq_sentence_vectorized = seq_sentence_vectorized.toarray()[0]
                a = 'break'



            except json.JSONDecodeError as err:
                print("process report {0}   JSONDecodeError error: {1}".format(sequence_file_path, err))
                pass
            except:
                print("__getitem__: {0}, error: {1}".format(sequence_file_path, sys.exc_info()))
                pass

        syscalls_data = {
            "index": item,
            "sequence": seq_sentence_vectorized,
            #"sequence": syscalls_sequence, #np.array(syscalls_sequence, dtype=np.int64), #torch.Tensor(syscalls_sequence),
            "label": syscalls_label #numpy.int64(syscalls_label) # not working! int and to list..
        }
        return syscalls_data


class CuckooSCNNCSVDataset(Dataset):
    def __init__(self, arguments, ds_type, on_memory=False):
        self.log = get_logger(__name__)
        self.data_path = os.path.join(arguments.dataset_path, 'reports_from_csv') # ds_type) # no needed for splitted folders
        self.min_length = arguments.min_sequence_length
        self.max_length = arguments.max_sequence_length
        self.on_memory = on_memory
        self.max_syscalls_types = arguments.categories_input_layer
        self.balaced_order_csv_path = os.path.join(arguments.csv_dir, (ds_type+"_balaced_order.csv"))
        import pickle
        self.syscall_mapping = pickle.load(
            open(os.path.join(arguments.csv_dir, "syscall_mapping1.pickle"), "rb"))

        # use csv for balanced batch only in the train dataset:
        # if ds_type == TRAIN:
        #     self.train_df = pd.read_csv(self.csv_path)
        #     self.sequences = [os.path.join(self.data_path, row['hash']) for index, row in self.train_df.iterrows()
        #                       if os.path.isfile(os.path.join(self.data_path, row['hash']))]
        #     self.sequences_count = len(self.sequences)
        # else:
        #     # NIR: fixed name -> full path
        #     # self.sequences = [name for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]
        #     self.sequences = [os.path.join(self.data_path, name) for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]
        #     self.sequences_count = len(self.sequences)
        self.balaced_order_df = pd.read_csv(self.balaced_order_csv_path)
        self.sequences = [os.path.join(self.data_path, row['hash']) for _, row in self.balaced_order_df.iterrows()
                          if os.path.isfile(os.path.join(self.data_path, row['hash']))]
        self.sequences_count = len(self.sequences)
        self.scores = [row['score'] for _, row in self.balaced_order_df.iterrows()]

        #if syscall_mapping == None:
        #self.syscall_mapping = {}
        #else:
        #    self.syscall_mapping = syscall_mapping
        self.syscall_counter = 0

        self.log.info(f"Initialized CuckooSCNNDataset instance object for dataset type: {ds_type} of size: {self.sequences_count}.")

    def __len__(self):
        return self.sequences_count

    def __getitem__(self, item):
        return None if self.on_memory else self.__load_cuckoo_syscalls_sequence(item, self.sequences[item], self.scores[item])

    def __load_cuckoo_syscalls_sequence(self, item, sequence_file_path, score):
        syscalls_sequence = []
        syscalls_label = 0

        # score = 0
        # labels_dict_Path = Path(sequence_file_path).parent.parent / "labels.json"
        # with open(labels_dict_Path, 'rb') as file_handle:
        #     labels_dict = json.load(file_handle)
        #     score = numpy.float64(labels_dict[Path(sequence_file_path).name]['score'])

        score = self.scores[item]
        syscalls_label = 1 if score > 5 else 0

        with open(sequence_file_path, 'rb') as file_handle:
            try:
                # 1. process report file to json
                cuckoo_report = json.load(file_handle)

                # 2. extract sorted calls by time:
                procs_list = cuckoo_report['behavior']['processes']
                extracted_procs_dict_list = []
                for proc_dict in procs_list:
                    calls_list = proc_dict["calls"]
                    extracted_proc_dict_list = []
                    for call_dict in itertools.islice(calls_list, 0, self.max_length):
                        timestamp_str = call_dict['timestamp']
                        # if to syscall-timestamp is not valid, skip this syscall to the next one.
                        try:
                            date_time_obj = datetime.strptime(timestamp_str, '%Y%m%d%H%M%S.%f')
                        except:
                            continue
                        syscall_str = call_dict['api']
                        extracted_calls_times_dict = {}
                        extracted_calls_times_dict['date_time_obj'] = date_time_obj
                        extracted_calls_times_dict['syscall_str'] = syscall_str
                        extracted_proc_dict_list.append(extracted_calls_times_dict)
                    extracted_procs_dict_list.extend(extracted_proc_dict_list)
                extracted_procs_dict_list = sorted(extracted_procs_dict_list,
                                                   key=lambda call: call['date_time_obj'],
                                                   reverse=False)[0:self.max_length]

                # 3. convert syscalls string(s) to int(s)
                for syscall in (d['syscall_str'] for d in extracted_procs_dict_list):
                    if syscall in self.syscall_mapping:
                        syscall_code = self.syscall_mapping[syscall]
                    else:
                        import operator
                        max_syscall_id = max(self.syscall_mapping.items(), key=operator.itemgetter(1))[1]
                        syscall_code = max_syscall_id + 1
                        # self.syscall_counter += 1
                        # if self.syscall_counter > self.max_syscalls_types:
                        #     raise SystemError(
                        #         f"Exceeded maximum syscall types of {self.max_syscalls_types}, please increase \"categories_input_layer\" argument.")
                        # # NIR: fixed - adding the syscall mapping to the dict:
                        # self.syscall_mapping[syscall] = self.syscall_counter
                        # syscall_code = self.syscall_counter
                    syscalls_sequence.append(syscall_code)

            except json.JSONDecodeError as err:
                print("process report {0}   JSONDecodeError error: {1}".format(report_file, err))
                pass
            except:
                print("process_reports_file: {0}, error: {1}".format(report_file, sys.exc_info()))
                pass

        # padding each seq with zeros until max_seq_len = 10,000
        seq_length = len(syscalls_sequence)
        syscalls_sequence.extend([0] * (self.max_length - seq_length))

        syscalls_data = {
            "index": item,
            "sequence": syscalls_sequence, #np.array(syscalls_sequence, dtype=np.int64), #torch.Tensor(syscalls_sequence),
            "label": syscalls_label, #numpy.int64(syscalls_label) # not working! int and to list..
            "seq_length": seq_length
        }
        return syscalls_data


class CuckooSCNNDataset(Dataset):
    def __init__(self, arguments, ds_type, on_memory=False):
        self.log = get_logger(__name__)
        self.data_path = os.path.join(arguments.dataset_path, ds_type)
        self.min_length = arguments.min_sequence_length
        self.max_length = arguments.max_sequence_length
        self.on_memory = on_memory
        self.max_syscalls_types = arguments.categories_input_layer

        # NIR: fixed name -> full path
        # self.sequences = [name for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]
        self.sequences = [os.path.join(self.data_path, name) for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]

        self.sequences_count = len(self.sequences)

        self.syscall_mapping = {}
        self.syscall_counter = 0

        self.log.info(f"Initialized CuckooSCNNDataset instance object for dataset type: {ds_type} of size: {self.sequences_count}.")

    def __len__(self):
        return self.sequences_count

    def __getitem__(self, item):
        return None if self.on_memory else self.__load_cuckoo_syscalls_sequence(item, self.sequences[item])

    def __load_cuckoo_syscalls_sequence(self, item, sequence_file_path):
        syscalls_sequence = []
        syscalls_label = 0
        score = 0

        labels_dict_Path = Path(sequence_file_path).parent.parent / "labels.json"
        with open(labels_dict_Path, 'rb') as file_handle:
            labels_dict = json.load(file_handle)
            score = numpy.float64(labels_dict[Path(sequence_file_path).name]['score'])
        syscalls_label = 1 if score > 5 else 0

        with open(sequence_file_path, 'rb') as file_handle:
            try:
                # 1. process report file to json
                cuckoo_report = json.load(file_handle)

                # 2. extract sorted calls by time:
                procs_list = cuckoo_report['behavior']['processes']
                extracted_procs_dict_list = []
                for proc_dict in procs_list:
                    calls_list = proc_dict["calls"]
                    extracted_proc_dict_list = []
                    for call_dict in itertools.islice(calls_list, 0, self.max_length):
                        timestamp_str = call_dict['timestamp']
                        # if to syscall-timestamp is not valid, skip this syscall to the next one.
                        try:
                            date_time_obj = datetime.strptime(timestamp_str, '%Y%m%d%H%M%S.%f')
                        except:
                            continue
                        syscall_str = call_dict['api']
                        extracted_calls_times_dict = {}
                        extracted_calls_times_dict['date_time_obj'] = date_time_obj
                        extracted_calls_times_dict['syscall_str'] = syscall_str
                        extracted_proc_dict_list.append(extracted_calls_times_dict)
                    extracted_procs_dict_list.extend(extracted_proc_dict_list)
                extracted_procs_dict_list = sorted(extracted_procs_dict_list,
                                                   key=lambda call: call['date_time_obj'],
                                                   reverse=False)[0:self.max_length]

                # 3. convert syscalls string(s) to int(s)
                for syscall in (d['syscall_str'] for d in extracted_procs_dict_list):
                    if syscall not in self.syscall_mapping:
                        self.syscall_counter += 1
                        if self.syscall_counter > self.max_syscalls_types:
                            raise SystemError(
                                f"Exceeded maximum syscall types of {self.max_syscalls_types}, please increase \"categories_input_layer\" argument.")
                        # NIR: fixed - adding the syscall mapping to the dict:
                        self.syscall_mapping[syscall] = self.syscall_counter
                        syscall_code = self.syscall_counter
                    else:
                        syscall_code = self.syscall_mapping[syscall]
                    syscalls_sequence.append(syscall_code)

            except json.JSONDecodeError as err:
                print("process report {0}   JSONDecodeError error: {1}".format(report_file, err))
                pass
            except:
                print("process_reports_file: {0}, error: {1}".format(report_file, sys.exc_info()))
                pass

        syscalls_data = {
            "index": item,
            "sequence": syscalls_sequence, #np.array(syscalls_sequence, dtype=np.int64), #torch.Tensor(syscalls_sequence),
            "label": syscalls_label #numpy.int64(syscalls_label) # not working! int and to list..
        }
        return syscalls_data


class SeqSCNNDataset(Dataset):
    def __init__(self, arguments, ds_type, on_memory=False):
        self.log = get_logger(__name__)
        self.data_path = os.path.join(arguments.dataset_path, ds_type)
        self.min_length = arguments.min_sequence_length
        self.max_length = arguments.max_sequence_length
        self.on_memory = on_memory
        self.max_syscalls_types = arguments.categories_input_layer

        # NIR: fixed name -> full path
        # self.sequences = [name for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]
        self.sequences = [os.path.join(self.data_path, name) for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]

        self.sequences_count = len(self.sequences)

        self.syscall_mapping = {}
        self.syscall_counter = 0

        self.log.info(f"Initialized SeqSCNNDataset instance object for dataset type: {ds_type} of size: {self.sequences_count}.")

    def __len__(self):
        return self.sequences_count

    def __getitem__(self, item):
        return None if self.on_memory else self.__load_seq_syscalls_sequence(item, self.sequences[item])

    def __load_seq_syscalls_sequence(self, item, sequence_file_path):
        syscalls_sequence = []
        syscalls_label = 0
        score = 0

        score = float(os.path.basename(sequence_file_path).split(" ")[1].split(")")[0][1:])
        syscalls_label = 1 if score > 10.0 else 0

        # benign_level = 0.4
        # suspicious_level = 0.7
        # malicious_level = 1.0
        # """
        # Calculates the category of each sequence by it's report score
        # :param score: Value from 0.0 - 10.0
        # :return: The category of the score (Benign, Suspicious, Malicious)
        # """
        # if score < benign_level:
        #     return 0
        # elif score < suspicious_level:
        #     return 1
        # else:
        #     return 2

        # labels_dict_Path = Path(sequence_file_path).parent.parent / "labels.json"
        # with open(labels_dict_Path, 'rb') as file_handle:
        #     labels_dict = json.load(file_handle)
        #     score = numpy.float64(labels_dict[Path(sequence_file_path).name]['score'])
        # syscalls_label = 1 if score > 1 else 0

        with open(sequence_file_path, 'rb') as file_handle:
            for syscall_line in file_handle:
                syscall = syscall_line.decode("utf-8", "backslashreplace").split()[0]
                # NIR: fixed the opposite logic
                # if parsed_line["syscall"] in self.syscall_mapping:
                if syscall not in self.syscall_mapping:
                    self.syscall_counter += 1
                    if self.syscall_counter > self.max_syscalls_types:
                        raise SystemError(f"Exceeded maximum syscall types of {self.max_syscalls_types}, please increase \"categories_input_layer\" argument.")
                    # NIR: fixed - adding the syscall mapping to the dict:
                    self.syscall_mapping[syscall] = self.syscall_counter
                    syscall_code = self.syscall_counter
                else:
                    syscall_code = self.syscall_mapping[syscall]
                syscalls_sequence.append(syscall_code)
                if len(syscalls_sequence) >= self.max_length:
                    self.log.debug(f"Loaded syscalls sequence exceeds the maximum lenght of {self.max_length}, interrupted the sequel.")
                    break
        syscalls_data = {
            "index": item,
            "sequence": syscalls_sequence, # torch.FloatTensor(syscalls_sequence),
            "label": syscalls_label # numpy.int64(syscalls_label) # not working! int and to list..
        }
        return syscalls_data


class BineeSCNNDataset(Dataset):
    def __init__(self, arguments, ds_type, on_memory=False):
        self.log = get_logger(__name__)
        self.data_path = os.path.join(arguments.dataset_path, ds_type)
        self.min_length = arguments.min_sequence_length
        self.max_length = arguments.max_sequence_length
        self.on_memory = on_memory
        self.max_syscalls_types = arguments.categories_input_layer

        # NIR: fixed name -> full path
        # self.sequences = [name for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]
        self.sequences = [os.path.join(self.data_path, name) for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]

        self.sequences_count = len(self.sequences)

        self.syscall_mapping = {}
        self.syscall_counter = 0

        self.log.info(f"Initialized BineeSCNNDataset instance object for dataset type: {ds_type} of size: {self.sequences_count}.")

    def __len__(self):
        return self.sequences_count

    def __getitem__(self, item):
        return None if self.on_memory else self.__load_binee_syscalls_sequence(self.sequences[item])

    def __load_binee_syscalls_sequence(self, sequence_file_path):
        syscalls_sequence = []
        syscalls_label = 0
        score = 0

        labels_dict_Path = Path(sequence_file_path).parent.parent / "labels.json"
        with open(labels_dict_Path, 'rb') as file_handle:
            labels_dict = json.load(file_handle)
            score = numpy.float64(labels_dict[Path(sequence_file_path).name]['score'])
        syscalls_label = 1 if score > 1 else 0

        with open(sequence_file_path, 'rb') as file_handle:
            for syscall_line in file_handle:
                parsed_line = self.__parse_binee_syscall_line(syscall_line.decode("utf-8", "backslashreplace"))
                # NIR: fixed the opposite logic
                # if parsed_line["syscall"] in self.syscall_mapping:
                if parsed_line["syscall"] not in self.syscall_mapping:
                    self.syscall_counter += 1
                    if self.syscall_counter > self.max_syscalls_types:
                        raise SystemError(f"Exceeded maximum syscall types of {self.max_syscalls_types}, please increase \"categories_input_layer\" argument.")
                    # NIR: fixed - adding the syscall mapping to the dict:
                    self.syscall_mapping[parsed_line["syscall"]] = self.syscall_counter
                    parsed_line["syscall_code"] = self.syscall_counter
                else:
                    parsed_line["syscall_code"] = self.syscall_mapping[parsed_line["syscall"]]
                syscalls_sequence.append(parsed_line["syscall_code"])
                if len(syscalls_sequence) >= self.max_length:
                    self.log.debug(f"Loaded syscalls sequence exceeds the maximum lenght of {self.max_length}, interrupted the sequel.")
                    break
        syscalls_data = {
            "sequence": torch.FloatTensor(syscalls_sequence),
            "label": numpy.int64(syscalls_label) # not working! int and to list..
        }
        return syscalls_data

    def  __parse_binee_syscall_line(self, syscall_line):
        parsed_line = {}

        # NIR: added support in not-implemented hooks: [1] 0x213f93f0:  **setlocale**() = 0xb7fefe90
        if "**" in syscall_line:
            split_index = syscall_line.index("(")
            split_text = syscall_line[:split_index].split(" ")
            parsed_line["syscall"] = split_text[3].strip( '*' )
        else: # [1] 0x20101060: F GetModuleHandleW(lpModuleName = '') = 0x400000
            split_index = syscall_line.index("(")
            split_text = syscall_line[:split_index].split(" ")
            parsed_line["address"] = split_text[1][:-1]
            parsed_line["type"] = split_text[2]
            parsed_line["syscall"] = split_text[3]
            split_text = syscall_line[split_index + 1:].split(") = ")
            parsed_line["return"] = split_text[1]
            #parsed_line["args"] = self.__parse_binee_syscall_args(split_text[0].split(", ")) # not always exists?

        return parsed_line

    def __parse_binee_syscall_args(self, syscall_args):
        if len(syscall_args) == 0:
            return None
        else:
            args = {}
            for argument in syscall_args:
                arg_name_value = syscall_args[0].split(" = ")
                args[arg_name_value[0]] = arg_name_value[1]
            return args


class RandomSCNNDataset(Dataset):
    def __init__(self, arguments, ds_type, on_memory=False):
        self.log = log
        self.sequences_count = arguments.reports_count
        self.min_length = arguments.min_sequence_length
        self.max_length = arguments.max_sequence_length
        self.on_memory = on_memory
        self.max_syscalls_types = arguments.categories_input_layer
        self.portion = arguments.train_portion if ds_type is TRAIN else arguments.valid_portion if ds_type is VALIDATION else arguments.test_portion
        
        self.sequences = self.__generate_random_sequences()
        self.sequences_count = len(self.sequences)

        self.log.info(f"Initialized RandomSCNNDataset instance object for dataset type: {ds_type} of size: {self.sequences_count}.")

    def __len__(self):
        return self.sequences_count

    def __getitem__(self, item):
        return self.sequences[item]

    def __generate_random_sequences(self):
        # calculate the dataset portion size
        sequences_count = int(self.sequences_count * self.portion)

        # create a list of lengths for each sequence (let them have different lengths)
        sequence_lengths = np.random.randint(low=self.min_length, high=self.max_length + 1, size=sequences_count)

        # create the list of sequences according to the length
        syscalls_sequences = [np.random.randint(low=0, high=self.max_syscalls_types, size=seq_length) for seq_length in sequence_lengths]
        syscalls_sequences = [np.pad(sequence, (0, self.max_length - len(sequence)), mode='constant', constant_values=0) for sequence in syscalls_sequences]

        # create the score of the sequences
        syscalls_class = np.random.choice([0, 1], size=sequences_count, p=[0.8, 0.2]).astype(np.int64)

        # define and set the result dictionary
        sequences = {}
        for index in range(sequences_count):
            sequences[index] = {
                "index": index,
                "sequence": syscalls_sequences[index].astype(np.int64),
                "label": syscalls_class[index]
            }

        return sequences



#
# class CuckooSCNNDataset(Dataset):
#     def __init__(self, arguments, ds_type, on_memory=False):
#         self.log = get_logger(__name__)
#         self.data_path = os.path.join(arguments.dataset_path, ds_type)
#         self.min_length = arguments.min_sequence_length
#         self.max_length = arguments.max_sequence_length
#         self.on_memory = on_memory
#         self.max_syscalls_types = arguments.categories_input_layer
#
#         # NIR: fixed name -> full path
#         # self.sequences = [name for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]
#         self.sequences = [os.path.join(self.data_path, name) for name in os.listdir(self.data_path) if os.path.isfile(os.path.join(self.data_path, name))]
#
#         self.sequences_count = len(self.sequences)
#
#         self.syscall_mapping = {}
#         self.syscall_counter = 0
#
#         self.log.info(f"Initialized CuckooSCNNDataset instance object for dataset type: {ds_type} of size: {self.sequences_count}.")
#
#     def __len__(self):
#         return self.sequences_count
#
#     def __getitem__(self, item):
#         return None if self.on_memory else self.__load_cuckoo_syscalls_sequence(item, self.sequences[item])
#
#     def __load_cuckoo_syscalls_sequence(self, item, sequence_file_path):
#         syscalls_sequence = []
#         syscalls_label = 0
#         score = 0
#
#         labels_dict_Path = Path(sequence_file_path).parent.parent / "labels.json"
#         with open(labels_dict_Path, 'rb') as file_handle:
#             labels_dict = json.load(file_handle)
#             score = numpy.float64(labels_dict[Path(sequence_file_path).name]['score'])
#         syscalls_label = 1 if score > 5 else 0
#
#         with open(sequence_file_path, 'rb') as file_handle:
#             cuckoo_report = json.load(file_handle)
#             max_calls_process = max(cuckoo_report['behavior']['processes'], key=lambda coll: len(coll['calls']))
#             for call in max_calls_process["calls"]:
#                 if call["api"] not in self.syscall_mapping:
#                     self.syscall_counter += 1
#                     if self.syscall_counter > self.max_syscalls_types:
#                         raise SystemError(f"Exceeded maximum syscall types of {self.max_syscalls_types}, please increase \"categories_input_layer\" argument.")
#                     # NIR: fixed - adding the syscall mapping to the dict:
#                     self.syscall_mapping[call["api"]] = self.syscall_counter
#                     syscall_code = self.syscall_counter
#                 else:
#                     syscall_code = self.syscall_mapping[call["api"]]
#                 syscalls_sequence.append(syscall_code)
#                 if len(syscalls_sequence) >= self.max_length:
#                     self.log.debug(f"Loaded syscalls sequence exceeds the maximum lenght of {self.max_length}, interrupted the sequel.")
#                     break
#         syscalls_data = {
#             "index": item,
#             "sequence": syscalls_sequence, #np.array(syscalls_sequence, dtype=np.int64), #torch.Tensor(syscalls_sequence),
#             "label": syscalls_label #numpy.int64(syscalls_label) # not working! int and to list..
#         }
#         return syscalls_data
#
#
#         # for index in range(sequences_count):
#         #     sequences[index] = {
#         #         "index": index,
#         #         "sequence": syscalls_sequences[index].astype(np.int64),
#         #         "label": syscalls_class[index]
#         #     }
#
