import torch.nn as nn
import torch.nn.utils as utils
import torch.nn.functional as F
import torch

# from .attention import SCNNAttention
from utils.logger import *


log = get_logger(__name__)

class SCNN(nn.Module):
    def __init__(self, device, batch_size, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(SCNN, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.embed_size = 300
        self.hidden_size = 300
        self.MaxSeqLen = 10000

        # num_embeddings = Vocab-Size = 47
        self.embedding = nn.Embedding(num_embeddings=47+1, embedding_dim=self.embed_size)

        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0.5, num_layers=1, bidirectional=True, batch_first=True)

        self.layer_norm = nn.LayerNorm(self.hidden_size*2)

        self.linear = nn.Linear(self.hidden_size*2, 2) # bidirectional implies multiply by 2

        self.retry = 3

    def init_hidden(self, num_layers=1, batch_size=10):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

                # # input_sequence is already padded to MaxSeqLen = 1000
                # input_sequence_padded_pacekd = utils.rnn.pack_padded_sequence(input_sequence_padded,
                #                                                  seq_lengths,
                #                                                  batch_first=True,
                #                                                  enforce_sorted=False)
                # embedded = self.embedding(input_sequence_padded_pacekd.data)
                # embedded_packed = utils.rnn.PackedSequence(embedded, input_sequence_padded_pacekd.batch_sizes)

                embedded = self.embedding(input_sequence_padded)

                # input_sequence is already padded to MaxSeqLen = 1000
                embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                                                                seq_lengths,
                                                                batch_first=True,
                                                                enforce_sorted=False)

                hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
                self.gru.flatten_parameters()
                gru_out, hidden = self.gru(embedded_packed, hidden)

                gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)

                gru_out = self.layer_norm(gru_out)

                gru_out = gru_out.permute(1,2,0)


                #gru_out = torch.transpose(gru_out, 1, 2).contiguous()
                pooled = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)
                #print(gru_out.size())
                relued = F.relu(pooled)
                out = F.dropout(relued, p=0.5)

                return F.log_softmax(self.linear(out))

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)