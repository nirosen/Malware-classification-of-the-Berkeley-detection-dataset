import torch.nn as nn
import torch.nn.utils as utils
import torch.nn.functional as F
import torch

# from .attention import SCNNAttention
from utils.logger import *

log = get_logger(__name__)


# https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/
class Attention_from_kaggle(nn.Module):
    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):
        super(Attention_from_kaggle, self).__init__(**kwargs)

        self.supports_masking = True

        self.bias = bias
        self.feature_dim = feature_dim
        self.step_dim = step_dim
        self.features_dim = 0

        weight = torch.zeros(feature_dim, 1)
        nn.init.kaiming_uniform_(weight)
        self.weight = nn.Parameter(weight)

        if bias:
            self.b = nn.Parameter(torch.zeros(step_dim))

    def forward(self, x, mask=None):
        feature_dim = self.feature_dim
        step_dim = self.step_dim

        eij = torch.mm(
            x.contiguous().view(-1, feature_dim),
            self.weight
        ).view(-1, step_dim)

        if self.bias:
            eij = eij + self.b

        eij = torch.tanh(eij)
        a = torch.exp(eij)

        if mask is not None:
            a = a * mask

        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)

        weighted_input = x.permute(1,0,2) * torch.unsqueeze(a, -1) # x -> x.permute(1,0,2)
        return torch.sum(weighted_input, 1)


# %
import numpy as np
class Attention_from_liron(nn.Module):
    def __init__(self, input_dim, num_attn_heads, attn_dim, max_len, \
                 pe_dim=None, pe_use_method='concat', mask_val=-1e9):
        super().__init__()
        if not attn_dim * num_attn_heads == input_dim + \
               (0 if pe_use_method == 'add' else pe_dim):
            raise ValueError(f"The attention input dim {input_dim} isn't "
                             f"equal to the dimension of an attention head "
                             f"{attn_dim} times the number of attention heads "
                             f"{num_attn_heads}")

        self.input_dim = input_dim
        self.num_attn_heads = num_attn_heads
        self.attn_dim = attn_dim

        self.k_linear = nn.Linear(attn_dim, 1)
        self.v_linear = nn.Linear(attn_dim, attn_dim)
        self.out_linear = nn.Linear(input_dim, input_dim)

        self.max_len = max_len
        self.pe_dim = input_dim if pe_use_method == 'add' else pe_dim
        self.pe_use_method = pe_use_method

        self.pe_base = self.get_pe_base(max_len, self.pe_dim)

        self.attn_score_multiplier = 1 / (attn_dim ** 0.5)
        self.mask_val = mask_val
        return

    @staticmethod
    def get_pe_base(max_len, pe_dim):
        pe_base = torch.zeros(max_len, pe_dim).float().cuda()
        pe_base.require_grad = False
        position = torch.arange(0, max_len).float().unsqueeze(1)
        pe_div_term = (torch.arange(0, pe_dim, 2).float() * \
                       -(np.log(1e4) / pe_dim)).exp()
        pe_base[:, 0::2] = torch.sin(position * pe_div_term)
        pe_base[:, 1::2] = torch.cos(position * pe_div_term)
        return pe_base

    def combine_pe(self, x, batch_max_len, batch_size):
        pe = self.pe_base[:batch_max_len, :].unsqueeze(1)
        pe = pe.expand(-1, batch_size, -1)
        if self.pe_use_method == 'concat':
            return torch.cat([x, pe], dim=2)
        elif self.pe_use_method == 'add':
            return x + pe
        else:
            raise NotImplementedError("Unknown pe_use_method "
                                      f"{self.pe_use_method}")

    @staticmethod
    def get_len_mask(lens, mask_dtype=torch.uint8):
        len_mask = torch.zeros((len(lens), max(lens)), dtype=mask_dtype).cuda()
        for i, cur_seq_len in enumerate(lens):
            len_mask[i, cur_seq_len:] = 1
        return len_mask.unsqueeze(1).unsqueeze(2)

    def apply_attention(self, x, lens):
        if max(lens) > self.max_len:
            raise ValueError("The current sequence is longer than the "
                             f"maximal sequence length: {max(lens)} > "
                             f"{self.max_len}")
        batch_max_len, batch_size = x.size()[:2]
        x = self.combine_pe(x, batch_max_len, batch_size)

        x_attn_reshaped = x.view(batch_size, -1, self.num_attn_heads, \
                                 self.attn_dim).transpose(1, 2)
        k = self.k_linear(x_attn_reshaped)
        v = self.v_linear(x_attn_reshaped)

        unmasked_attn_scores = k.transpose(-2, -1)
        attn_scores = unmasked_attn_scores.masked_fill(self.get_len_mask(lens), \
                                                       self.mask_val)
        normlized_attn_weights = F.softmax(attn_scores, dim=-1)
        attended = torch.matmul(normlized_attn_weights, v).squeeze(1)
        attended = attended.transpose(1, 2).contiguous().view(-1, self.attn_dim * self.num_attn_heads)
        return attended

    def forward(self, x, lens):
        attended = self.apply_attention(x, lens)
        return attended
# %

class SCNN(nn.Module):
    def __init__(self, device, batch_size, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(SCNN, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.embed_size = 300
        self.hidden_size = 300
        self.MaxSeqLen = 10000

        # num_embeddings = Vocab-Size = 45
        self.embedding = nn.Embedding(num_embeddings=45+1, embedding_dim=self.embed_size)

        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        self.lstm = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0, num_layers=1, bidirectional=True, batch_first=True)

        self.gru = nn.GRU(input_size=int(self.hidden_size*2), hidden_size=int(self.hidden_size/2),
                          dropout=0, num_layers=1, bidirectional=True, batch_first=True)

        self.attention_layer = Attention_from_kaggle(self.hidden_size, self.MaxSeqLen)

        #self.attention_layer = Attention_from_liron(input_dim=self.hidden_size*2,
        #                                            num_attn_heads=5, attn_dim=122,
        #                                            max_len=self.MaxSeqLen, pe_dim=10)

        #self.attention_layer = nn.MultiheadAttention(embed_dim=self.hidden_size * 2, num_heads=1, dropout=0.5)

        self.linear1 = nn.Linear(self.hidden_size, int(self.hidden_size/2)) # bidirectional implies multiply by 2 + add 10 for pe_dim=10
        self.linear2 = nn.Linear(int(self.hidden_size/2), 2) # bidirectional implies multiply by 2 + add 10 for pe_dim=10

    def init_hidden(self, num_layers=1, batch_size=10):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None):
        try:
            # padding each seq with zeros until max_seq_len = 10,000
            input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
            input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

            embedded = self.embedding(input_sequence_padded)

            # input_sequence is already padded to MaxSeqLen = 1000
            embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                                                            seq_lengths,
                                                            batch_first=True,
                                                            enforce_sorted=False)

            #hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0])).cuda()
            #self.lstm.flatten_parameters()
            lstm_out, hidden = self.lstm(embedded_packed)

            #hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0])).cuda()
            #self.gru.flatten_parameters()
            gru_out, hidden = self.gru(lstm_out)

            #gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)
            #gru_out = gru_out.permute(1,0,2)#(1,2,0)

            #attented = self.attention_layer(gru_out.permute(1,0,2), seq_lengths) self.attention_layer(utils.rnn.pad_packed_sequence(gru_out)[0], seq_lengths)
            #attented = self.attention_layer(utils.rnn.pad_packed_sequence(gru_out)[0], seq_lengths) # liron

            self.attention_layer = Attention_from_kaggle(self.hidden_size, int(max(seq_lengths))).cuda()
            attented = self.attention_layer(utils.rnn.pad_packed_sequence(gru_out)[0])


            ## before attention:
            # gru_out = gru_out.permute(1,2,0)
            # pooled = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)
            # #print(gru_out.size())
            # relued = F.relu(pooled)
            # out = F.dropout(relued, p=0.5)

            lin1 = self.linear1(attented)

            relued = F.relu(lin1)
            #dropped = F.dropout(relued, p=0.5)

            return F.log_softmax(self.linear2(relued))

        except Exception as ex:
            print(ex)
            print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)