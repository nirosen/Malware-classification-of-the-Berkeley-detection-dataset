import torch.nn as nn
import torch.nn.utils as utils
import torch.nn.functional as F
import torch

# from .attention import SCNNAttention
from utils.logger import *


log = get_logger(__name__)

# import numpy as np
# class Attention(nn.Module):
#     def __init__(self, device, input_dim, num_attn_heads, attn_dim, max_len, mask_val=-1e9):
#         super().__init__()
#         self.device = device
#         if not attn_dim * num_attn_heads == input_dim:
#             raise ValueError(f"The attention input dim {input_dim} isn't "
#                              f"equal to the dimension of an attention head "
#                              f"{attn_dim} times the number of attention heads "
#                              f"{num_attn_heads}")
#
#         self.input_dim = input_dim
#         self.num_attn_heads = num_attn_heads
#         self.attn_dim = attn_dim
#
#         self.k_linear = nn.Linear(attn_dim, attn_dim)
#         self.v_linear = nn.Linear(attn_dim, attn_dim)
#         self.out_linear = nn.Linear(input_dim, input_dim)
#
#         self.max_len = max_len
#         pe_base = torch.zeros(max_len, input_dim).float().to(self.device)
#         pe_base.require_grad = False
#         position = torch.arange(0, max_len).float().unsqueeze(1)
#         pe_div_term = (torch.arange(0, input_dim, 2).float() * -(np.log(1e4) / input_dim)).exp()
#         pe_base[:, 0::2] = torch.sin(position * pe_div_term)
#         pe_base[:, 1::2] = torch.cos(position * pe_div_term)
#         self.pe_base = pe_base
#
#         self.attn_score_multiplier = 1 / (attn_dim ** 0.5)
#         self.mask_val = mask_val
#         return
#
#     def with_pe(self, x, batch_max_len, batch_size):
#         pe = torch.unsqueeze(self.pe_base[:batch_max_len, :], 1).expand(-1, batch_size, -1)
#         return x + pe
#
#     def get_len_mask(self, lens, mask_dtype=torch.uint8):
#         len_mask = torch.zeros((len(lens), max(lens)), dtype=mask_dtype).to(self.device)
#         for i, cur_seq_len in enumerate(lens):
#             len_mask[i, cur_seq_len:] = 1
#         return len_mask.unsqueeze(1).unsqueeze(2)
#
#     def apply_attention(self, x, lens):
#         if max(lens) > self.max_len:
#             raise ValueError("The current sequence is longer than the "
#                              f"maximal sequence length: {max(lens)} > "
#                              f"{self.max_len}")
#         batch_max_len, batch_size = x.size()[:2]
#         x = self.with_pe(x, batch_max_len, batch_size)
#
#         x_attn_reshaped = x.view(batch_size, -1, self.num_attn_heads, \
#                                  self.attn_dim).transpose(1, 2)
#         k = self.k_linear(x_attn_reshaped).squeeze(-1)
#         v = self.v_linear(x_attn_reshaped)
#
#         attn_scores = k.transpose(-2, -1).masked_fill(self.get_len_mask(lens), self.mask_val)
#         normlized_weights = F.softmax(attn_scores, dim=-1)
#         attended = torch.matmul(normlized_weights, v).squeeze(1)
#         output = attended.transpose(1, 2).contiguous().view(-1, self.attn_dim * self.num_attn_heads)
#         return output
#
#     def forward(self, x, lens):
#         attended = self.apply_attention(x, lens)
#         return attended
#


class Attention_Kaggle(nn.Module):
    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):
        super().__init__()

        self.supports_masking = True

        self.bias = bias
        self.feature_dim = feature_dim
        self.step_dim = step_dim
        self.features_dim = 0

        weight = torch.zeros(feature_dim, 1)
        nn.init.kaiming_uniform_(weight)
        self.weight = nn.Parameter(weight)

        if bias:
            self.b = nn.Parameter(torch.zeros(step_dim))

    def forward(self, x, mask=None):
        feature_dim = self.feature_dim
        step_dim = self.step_dim

        eij = torch.mm(
            x.contiguous().view(-1, feature_dim),
            self.weight
        ).view(-1, step_dim)

        if self.bias:
            eij = eij + self.b

        eij = torch.tanh(eij)
        a = torch.exp(eij)

        if mask is not None:
            a = a * mask

        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)

        weighted_input = x * torch.unsqueeze(a, -1)
        return torch.sum(weighted_input, 1)


class SCNN(nn.Module):
    def __init__(self, device, batch_size, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(SCNN, self).__init__()
        self.device = device
        self.batch_size = batch_size

        drp = 0.1
        self.embed_size = 32
        self.MaxSeqLen = 10

        self.embedding = nn.Embedding(num_embeddings=45+1, embedding_dim=self.embed_size, padding_idx=0) # num_embeddings = Vocab-Size = 45 and 1 for padding 0
        self.gru = nn.GRU(input_size=self.embed_size, hidden_size=64) #input_size=128, hidden_size=64, num_layers=1, batch_first=True, dropout=0, bidirectional=True)

        #self.attention_liron = Attention(device=self.device, input_dim=64, num_attn_heads=8, attn_dim=8, max_len=1000)

        self.attention_kaggle = Attention_Kaggle(feature_dim=64, step_dim=self.MaxSeqLen)

        #self.normalize = nn.BatchNorm1d(num_features=64*2)
        self.linear = nn.Linear(64, 2) # bidirectional implies multiply 64 by 2

        # Drop some of the nodes to increase robustness in training
        self.dropout = nn.Dropout(0.5)

    def init_hidden(self):
        return None # torch.autograd.Variable(torch.randn(1*2, self.batch_size, 64)).cuda() # bidirectional implies (1*2, 10, 64)

    def forward(self, input_sequence, seq_lengths, hn=None):
        try:
            # padding each seq with zeros until max_seq_len = 10,000
            input_sequence_padded = torch.zeros(input_sequence.shape[0], 1000).to(self.device)
            input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence

            # input_sequence is already padded to MaxSeqLen = 1000
            PackedSequence = utils.rnn.pack_padded_sequence(input_sequence_padded.long(),
                                                                     seq_lengths,
                                                                     batch_first=True,
                                                                     enforce_sorted=False)

            embedded = utils.rnn.PackedSequence(self.embedding.forward(PackedSequence.data), PackedSequence.batch_sizes)#.permute(0, 2, 1)  seq_lengths, batch_first=True, enforce_sorted=False)

            #predicted = self.gru.forward(embedded)[0]  # [0] selects the sequence of outputs, not just the output from the last step

            predicted = torch.nn.utils.rnn.pad_packed_sequence(self.gru.forward(embedded)[0])[0].permute(1, 0, 2)
            PackedPredicted = utils.rnn.pack_padded_sequence(predicted,
                                                            seq_lengths,
                                                            batch_first=True,
                                                            enforce_sorted=False)

            padded_predicted = utils.rnn.pad_packed_sequence(PackedPredicted)

            #attended = self.attention.forward(padded_predicted[0], padded_predicted[1].to(self.device))
            attended = self.attention_kaggle(padded_predicted[0])

            # output, hn = self.gru(embedded_PackedSequence, hn)
            # unpacked_output, unpacked_output_len = torch.nn.utils.rnn.pad_packed_sequence(output)
            # unpacked_output = unpacked_output.permute(1,0,2)
            #
            # unpacked_output = self.normalize(unpacked_output[:,-1,:])
            #
            # # Perform final linear and softmax
            # output = self.dropout(unpacked_output) # unpacked_output[:,-1,:].squeeze()) # output[:,-1,:].squeeze() => (10,1000,128) -> (10,128)
            # output = F.log_softmax(self.linear(output)) # torch.cat dim=1

            #output = torch.nn.utils.rnn.pad_packed_sequence(self.gru.forward(embedded)[0])[0].permute(1, 0, 2)

            output = F.log_softmax(self.linear(output[:,-1,:])) # attended shape is (80,64) instead of (10,64) ???
            return output, hn

        except Exception as ex:
            print(ex)
            print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)


            # output, hn = self.lstm(embedded)
            #normalized_output = self.normalize(output)
            #predicted_log_probabilities = F.log_softmax(self.linear(normalized_output), dim=1)
