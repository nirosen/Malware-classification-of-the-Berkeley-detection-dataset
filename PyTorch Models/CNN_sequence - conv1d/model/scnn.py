import torch.nn as nn
import torch.nn.utils as utils
import torch.nn.functional as F
import torch

# from .attention import SCNNAttention
from utils.logger import *


log = get_logger(__name__)


class SCNN(nn.Module):
    def __init__(self, device, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(SCNN, self).__init__()
        self.device = device
        self.retry = 3

        ## CNN of seq with CONVD:
        # Create word embeddings from the input words
        self.embedding = nn.Embedding(45+1, 100) # Vocab-Size 45
        # Specify convolutions with filters of different sizes (fs)
        self.convs = nn.ModuleList([nn.Conv1d(in_channels=100,
                                              out_channels=100,
                                              kernel_size=k)
                                    for k in [3]])
        # Add a fully connected layer for final predicitons
        #self.linear = nn.Linear(len([3]) * 100, 2)
        self.linear = nn.Linear(45 * 10000, 2)
        # Drop some of the nodes to increase robustness in training
        #self.dropout = nn.Dropout(0.5)

    def forward(self, input_sequence):
        for try_count in range(self.retry):
            try:


                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], 10000).to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1] ] = input_sequence

                # 1-hot instead of embedding:
                input_sequence_onehot = F.one_hot(input_sequence_padded.long(), 45)
                # F.avg_pool1d(input_sequence_onehot.float(), input_sequence_onehot.shape[2]).squeeze(2)


                # Get word embeddings and formt them for convolutions
                # Conv1d takes in (batch, channels, seq_len), but raw embedded is (batch, seq_len, channels)
                #embedded = self.embedding(input_sequence_padded.long()).permute(0, 2, 1)

                # Perform convolutions
                #conveds = [conv(embedded)
                #           for conv in self.convs]

                # Perform avg pooling on original seq len (manual and API are the same)
                # manual avg pooling
                #pooleds = [conved[:,:,:input_sequence.shape[1]].sum(2).div(input_sequence.shape[1])
                #          for conved in conveds]
                #pooleds = [F.avg_pool1d(conved[:,:,:input_sequence.shape[1]], input_sequence.shape[1]).squeeze(2)
                #          for conved in conveds]
                # API avg pooling
                #relueds = [F.relu(pooled)
                #          for pooled in pooleds]

                # Perform Dropout
                #cat = self.dropout(torch.cat(conved, dim=1))

                # Perform final linear and softmax
                return F.log_softmax(self.linear(input_sequence_onehot.view(input_sequence_onehot.shape[0], -1).float())) # torch.cat dim=1

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)
                continue


                # output, hn = self.lstm(embedded)
                #normalized_output = self.normalize(output)
                #predicted_log_probabilities = F.log_softmax(self.linear(normalized_output), dim=1)
