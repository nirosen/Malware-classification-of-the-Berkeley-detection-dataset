import argparse
import time

from torch.utils.data import DataLoader

from model.scnn import SCNN
from dataset.dataset import *
from dataset.loader import *
from trainer.model_trainer import *
from utils.logger import *
from utils.constants import *

from analysis.visualization import plot_simple_loss_graph

from datetime import datetime

# define global logger
log = get_logger(__name__)

def validate_arguments(arguments):
    valid = True
    if arguments.dataset_type not in ["random", "binee", "cuckoo", "cuckoo_csv", "cuckoo_csv_vectorizer", "cuckoo_csv_words", "seq"]:
        log.warn("Invalid argument \"dataset_type\", valid values are (random, binee, cuckoo, seq).")
        valid = False
    return valid


def initialize_device(with_cuda: bool):
    """
    Initialize torch engine device for the model and dataset
    :param with_cuda: (bool) Whether or not to use CUDA with GPU
    :return: Initialized torch device
    """
    if with_cuda:
        # clear GPU memory
        torch.cuda.empty_cache()

        # define the CUDA device to run on GPU
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    else:
        # define CPU device
        device = torch.device("cpu")
    return device


def print_arguments(start_time, device: torch.device, arguments):
    """

    :param start_time:
    :param device:
    :param arguments:
    :return:
    """
    log.info(f"Starting pre-processing time: {start_time}")
    log.info(' '.join(sys.argv))
    log.info(device)

    # print spec for run
    log.info(f"Input Categories: {arguments.categories_input_layer}")
    log.info(f"Embedding Layer: {arguments.embedding_layer}")
    log.info(f"Hidden Layer: {arguments.hidden_layer}")
    log.info(f"Output Layer : {arguments.classes_output_layer}")
    log.info(f"Sequence size between {arguments.min_sequence_length} to {arguments.max_sequence_length}")
    log.info(f"Reports count: {arguments.reports_count}")
    # print(f"Dataset split portions: train({arguments.train_portion}) / valid({arguments.valid_portion}) / test({arguments.test_portion})")
    log.info(f"Batch size: {arguments.batch_size}")
    log.info(f"Batch count: {arguments.batch_count}")
    log.info(f"Epoch count: {arguments.epoch_count}")


def initialize_dataset_loaders():
    loader_factory = DatasetLoaderFactory()
    loader_factory.register_loader("random", RandomSCNNDataset)
    loader_factory.register_loader("binee", BineeSCNNDataset)
    loader_factory.register_loader("cuckoo", CuckooSCNNDataset)
    loader_factory.register_loader("seq", SeqSCNNDataset)
    loader_factory.register_loader("cuckoo_csv", CuckooSCNNCSVDataset)
    loader_factory.register_loader("cuckoo_csv_vectorizer", CuckooSCNNCSVVectorizerDataset)
    loader_factory.register_loader("cuckoo_csv_words", CuckooSCNNCSVWordsDataset)
    return loader_factory


def train_model():
    log.info("Staring SCNN model training")

    parser = argparse.ArgumentParser()
    parser.add_argument("-d", "--dataset_path", required=True, type=str, help="dataset directory path for scnn (with train, valid and test folders inside)")
    parser.add_argument("-f", "--dataset_type", required=True, type=str, help="type of the dataset to load for scnn (binee, random, cuckoo, seq)")
    parser.add_argument("-csv", "--csv_dir", required=False, type=str, help="path to the dir with csv-files containing the samples, one for each dataset (train, valid, test)")
    parser.add_argument("-n", "--normalize", required=False, type=bool, default=True, help="whether or not to normalize data in scnn")
    parser.add_argument("-t", "--train_portion", required=False, type=float, default=0.7, help="portion of the train data from the dataset")
    parser.add_argument("-v", "--valid_portion", required=False, type=float, default=0.15, help="portion of the validation data from the dataset")
    parser.add_argument("-s", "--test_portion", required=False, type=float, default=0.15, help="portion of the test data from the dataset")
    parser.add_argument("-r", "--learning_rate", required=False, type=float, default=3e-4, help="learning rate of the model")
    parser.add_argument("-l", "--loss_threshold", required=False, type=float, default=1e-4, help="loss threshold for early stopping of the training phase")
    parser.add_argument("-c", "--categories_input_layer", required=False, type=int, default=150, help="input layer size (override by input data after initial parsing)")
    parser.add_argument("-e", "--embedding_layer", required=False, type=int, default=256, help="embedding layer size of scnn")
    parser.add_argument("-i", "--hidden_layer", required=False, type=int, default=128, help="hidden layer size of scnn")
    # parser.add_argument("-a", "--attention_layer", required=True, type=int, default=?, help="?")
    parser.add_argument("-o", "--classes_output_layer", required=False, type=int, default=2, help="output classes count of scnn")
    parser.add_argument("-p", "--epoch_count", required=False, type=int, default=10, help="number of epochs")
    parser.add_argument("-b", "--batch_count", required=False, type=int, default=100, help="number of batches per epoch")
    parser.add_argument("-z", "--batch_size", required=False, type=int, default=10, help="amount of syscalls for each batch")
    parser.add_argument("-w", "--with_cuda", required=False, type=bool, default=True, help="training model with CUDA")
    parser.add_argument("-m", "--min_sequence_length", required=False, type=int, default=1000, help="minimum syscall sequence length")
    parser.add_argument("-x", "--max_sequence_length", required=False, type=int, default=1000, help="maximum syscall sequence length")
    parser.add_argument("-u", "--reports_count", required=False, type=int, default=1000, help="amount of syscall sequences for training")
    now = datetime.now()
    parser.add_argument("-out", "--output_dir", required=False, type=str, default=('output' + now.strftime("%d%m%Y_%H%M%S")), help="output directory for plots and cmdline")


    start_time = time.time()


    arguments = parser.parse_args()
    if not validate_arguments(arguments=arguments):
        log.info("Invalid arguments, terminating application.")
        exit(1)

    os.mkdir(arguments.output_dir)
    with open(os.path.join(arguments.output_dir, 'args.txt'), 'w') as file:
        file.write(' '.join(sys.argv))

    log.info("Parsed SCNN Arguments")
    device = initialize_device(arguments.with_cuda)

    log.info("Initialized CUDA Device")
    print_arguments(start_time=start_time, device=device, arguments=arguments)

    log.info("Initializer Dataset Loaders Factory")
    loader_factory = initialize_dataset_loaders()

    try:
        log.info(f"Get Dataset Loader: {arguments.dataset_type}")
        dataset_loader = loader_factory.get_dataset_loader(arguments.dataset_type)

        log.info("Loading Train Dataset")
        train_dataset = dataset_loader(arguments, TRAIN)

        log.info("Loading Validation Dataset")
        valid_dataset = dataset_loader(arguments, VALIDATION)

        log.info("Loading Test Dataset")
        test_dataset = dataset_loader(arguments, TEST)

        log.info("Creating Dataloader Object For The Datasets")

        # NIR: setting shuffle=False for train batch balance
        if arguments.csv_dir:
            shuffle_dataloader = False
        else:
            shuffle_dataloader = True
        train_dataloader = DataLoader(dataset=train_dataset, batch_size=arguments.batch_size, shuffle=shuffle_dataloader)
        validation_dataloader = DataLoader(dataset=valid_dataset, batch_size=arguments.batch_size, shuffle=shuffle_dataloader)
        test_dataloader = DataLoader(dataset=test_dataset, batch_size=arguments.batch_size, shuffle=shuffle_dataloader)
    except ValueError as ex:
        log.error(f"Invalid argument dataset_type: {arguments.dataset_type}.", ex)
        exit(1)

    log.info("Creating SCNN Model")
    scnn = SCNN(device, arguments.batch_size, arguments.categories_input_layer, arguments.embedding_layer, arguments.hidden_layer, arguments.classes_output_layer, arguments.normalize)

    log.info("Creating SCNN Trainer")
    trainer = SCNNTrainer(arguments=arguments, scnn=scnn, device=device, learning_rate=arguments.learning_rate,
                          train_dataloader=train_dataloader, validation_dataloader=validation_dataloader, test_dataloader=test_dataloader)

    log.info("Starting To Train The Model ...")
    train_avg_loss_list = []
    valid_avg_loss_list = []
    valid_avg_accuracy_list = []
    valid_avg_auc_list = []
    test_avg_loss_list = []
    test_avg_accuracy_list = []
    valid_metrics_arr = []
    for epoch in range(arguments.epoch_count):
        metrics = trainer.train(epoch)
        train_avg_loss_list.append(metrics['loss'])
        #trainer.save(epoch)
        metrics = trainer.validation(epoch)
        valid_avg_loss_list.append(metrics['loss'])
        valid_avg_accuracy_list.append(metrics['accuracy'])
        valid_avg_auc_list.append(metrics['auc'])
        valid_metrics_arr.append(metrics)

    # avg_loss using criterion: NLL(negative log likelihood) loss of the log probs classification result
    plot_simple_loss_graph(result_values=train_avg_loss_list, x_label="train epoch", y_label="avg NLL-loss", title="train loss", output_dir=arguments.output_dir)

    plot_simple_loss_graph(result_values=valid_avg_loss_list, x_label="valid epoch", y_label="avg NLL-loss", title="valid loss", output_dir=arguments.output_dir)
    plot_simple_loss_graph(result_values=valid_avg_accuracy_list, x_label="valid epoch", y_label="avg accuracy", title="valid accuracy", output_dir=arguments.output_dir)
    plot_simple_loss_graph(result_values=valid_avg_auc_list, x_label="valid epoch", y_label="avg auc", title="valid auc", output_dir=arguments.output_dir)

    metrics = trainer.test()
    #test_avg_loss_list.append(metrics['loss'])
    #test_avg_accuracy_list.append(metrics['accuracy'])
    print("/n/n ### final test metrics: ")
    print(metrics)
    # plot_simple_loss_graph(result_values=test_avg_loss_list, x_label="valid epoch", y_label="avg NLL-loss", title="test loss")
    # plot_simple_loss_graph(result_values=test_avg_accuracy_list, x_label="valid epoch", y_label="avg accuracy", title="test accuracy")

    # store valid and test metrics to pickle for later analysis and comparison vs SKLearn:
    import pickle
    path = os.path.join(arguments.output_dir, 'metrics_valid_epochs.pickle')
    pickle.dump(valid_metrics_arr, open(path, "wb"))
    path = os.path.join(arguments.output_dir, 'metrics_test_epoch.pickle')
    pickle.dump(metrics, open(path, "wb"))

if __name__ == '__main__':
    try:
        train_model()
    except Exception as ex:
        print(ex)
        logging.error(f"Exception __main__ Method: {ex}.", exc_info=True)
