import torch.nn as nn
import torch.nn.utils as utils
import torch.nn.functional as F
import torch

# from .attention import SCNNAttention
from utils.logger import *


log = get_logger(__name__)


class SCNN(nn.Module):
    def __init__(self, device, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(SCNN, self).__init__()
        self.device = device
        self.retry = 3

        ## CNN of seq with CONVD:
        # Create word embeddings from the input words
        self.embedding = nn.Embedding(45, 100) # Vocab-Size 45
        # Specify convolutions with filters of different sizes (fs)
        self.gru = nn.GRU(100, hidden_size)
        # Add a fully connected layer for final predicitons
        #self.linear = nn.Linear(len([3]) * 100, 2)
        self.linear = nn.Linear(hidden_size, 2)
        # Drop some of the nodes to increase robustness in training
        #self.dropout = nn.Dropout(0.5)

    def forward(self, input_sequence, seq_lengths):
        for try_count in range(self.retry):
            try:


                # padding each seq with zeros until max_seq_len = 10,000
                #input_sequence_padded = torch.zeros(input_sequence.shape[0], 10000).to(self.device)
                #input_sequence_padded[:, :input_sequence.shape[1] ] = input_sequence

                embedded = self.embedding(input_sequence.long())#.permute(0, 2, 1)

                # 1-hot instead of embedding:
                #input_sequence_onehot = F.one_hot(input_sequence.long(), 45)

                #input_sequence_onehot_PackedSequence = utils.rnn.PackedSequence(input_sequence_onehot.float(), seq_lengths)
                input_sequence_onehot_PackedSequence = utils.rnn.pack_padded_sequence(embedded.float(),
                                                                                      seq_lengths,
                                                                                      batch_first=True,
                                                                                      enforce_sorted=False)

                # output, hn = self.gru(input_sequence_onehot.view(input_sequence.shape[0],-1,100).float())
                output, hn = self.gru(input_sequence_onehot_PackedSequence)
                hn = hn.squeeze(0)
                #unpacked_output, unpacked_output_len = torch.nn.utils.rnn.pad_packed_sequence(output)

                # Get word embeddings and formt them for convolutions
                # Conv1d takes in (batch, channels, seq_len), but raw embedded is (batch, seq_len, channels)
                #embedded = self.embedding(input_sequence_padded.long()).permute(0, 2, 1)

                # Perform convolutions
                #conveds = [conv(embedded)
                #           for conv in self.convs]

                # Perform avg pooling on original seq len (manual and API are the same)
                # manual avg pooling
                #pooleds = [conved[:,:,:input_sequence.shape[1]].sum(2).div(input_sequence.shape[1])
                #          for conved in conveds]
                #pooleds = [F.avg_pool1d(conved[:,:,:input_sequence.shape[1]], input_sequence.shape[1]).squeeze(2)
                #          for conved in conveds]
                # API avg pooling
                #relueds = [F.relu(pooled)
                #          for pooled in pooleds]

                # Perform Dropout
                #cat = self.dropout(torch.cat(conved, dim=1))

                # Perform final linear and softmax
                return F.log_softmax(self.linear(hn)) # torch.cat dim=1

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)
                continue


                # output, hn = self.lstm(embedded)
                #normalized_output = self.normalize(output)
                #predicted_log_probabilities = F.log_softmax(self.linear(normalized_output), dim=1)
