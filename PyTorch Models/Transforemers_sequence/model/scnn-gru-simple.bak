import torch.nn as nn
import torch.nn.utils as utils
import torch.nn.functional as F
import torch

# from .attention import SCNNAttention
from utils.logger import *


log = get_logger(__name__)


class Attention(nn.Module):
    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):
        super(Attention, self).__init__(**kwargs)

        self.supports_masking = True

        self.bias = bias
        self.feature_dim = feature_dim
        self.step_dim = step_dim
        self.features_dim = 0

        weight = torch.zeros(feature_dim, 1)
        nn.init.kaiming_uniform_(weight)
        self.weight = nn.Parameter(weight)

        if bias:
            self.b = nn.Parameter(torch.zeros(step_dim))

    def forward(self, x, mask=None):
        feature_dim = self.feature_dim
        step_dim = self.step_dim

        eij = torch.mm(
            x.contiguous().view(-1, feature_dim),
            self.weight
        ).view(-1, step_dim)

        if self.bias:
            eij = eij + self.b

        eij = torch.tanh(eij)
        a = torch.exp(eij)

        if mask is not None:
            a = a * mask

        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)

        weighted_input = x * torch.unsqueeze(a, -1)
        return torch.sum(weighted_input, 1)

#
# class Attention_Net(nn.Module):
#     def __init__(self):
#         super(Attention_Net, self).__init__()
#         drp = 0.1
#         self.embedding = nn.Embedding(max_features, embed_size)
#         self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
#         self.embedding.weight.requires_grad = False
#
#         self.embedding_dropout = nn.Dropout2d(0.1)
#         self.lstm = nn.LSTM(embed_size, 128, bidirectional=True, batch_first=True)
#         self.lstm2 = nn.GRU(128 * 2, 64, bidirectional=True, batch_first=True)
#
#         self.attention_layer = Attention(feature_dim=128, step_dim=maxlen)
#
#         self.linear = nn.Linear(64*2, 64)
#         self.relu = nn.ReLU()
#         self.out = nn.Linear(64, 1)
#
#     def forward(self, x):
#         h_embedding = self.embedding(x)
#         h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))
#         h_lstm, _ = self.lstm(h_embedding)
#         h_lstm, _ = self.lstm2(h_lstm)
#         h_lstm_atten = self.attention_layer(h_lstm)
#         conc = self.relu(self.linear(h_lstm_atten))
#         out = self.out(conc)
#         return out
#

class SCNN(nn.Module):
    def __init__(self, device, batch_size, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(SCNN, self).__init__()
        self.device = device
        self.batch_size = batch_size

        #drp = 0.5
        self.embed_size = 300
        self.hidden_size = 256
        self.MaxSeqLen = 10000

        self.embedding = nn.Embedding(num_embeddings=45+1, embedding_dim=self.embed_size) # num_embeddings = Vocab-Size = 45
        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        #self.embedding.weight.requires_grad = False

        #self.embedding_dropout = nn.Dropout2d(0.1)

        #self.lstm = nn.LSTM(input_size=self.embed_size, hidden_size=128, bidirectional=True, batch_first=True)

        self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size, num_layers=3, bidirectional=False, batch_first=True)

        #self.attention_layer = Attention(feature_dim=self.hidden_size, step_dim=self.MaxSeqLen)

        self.linear = nn.Linear(self.hidden_size, 2)
        #self.relu = nn.ReLU()

        #self.normalize = nn.BatchNorm1d(64)
        #self.out = nn.Linear(64, 2)

        #self.normalize = nn.BatchNorm1d(num_features=64*2)
        # Drop some of the nodes to increase robustness in training
        #self.dropout = nn.Dropout(0.5)

    def forward(self, input_sequence, seq_lengths, hn=None):
        try:
            # padding each seq with zeros until max_seq_len = 10,000
            input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
            input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

            embedded = self.embedding(input_sequence_padded)

            # input_sequence is already padded to MaxSeqLen = 1000
            embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                                                            seq_lengths,
                                                            batch_first=True,
                                                            enforce_sorted=False)

            out, _ = self.gru(embedded_packed)

            out, lengths = utils.rnn.pad_packed_sequence(out, batch_first=False)

            # Since we are doing classification, we only need the last
            # output from RNN
            lengths = [l - 1 for l in lengths]
            last_output = out[lengths, range(len(lengths))]

            return F.log_softmax(self.linear(last_output))


            #embedded = utils.rnn.PackedSequence(self.embedding.forward(PackedSequence.data),
            #                                    PackedSequence.batch_sizes)  # .permute(0, 2, 1)  seq_lengths, batch_first=True, enforce_sorted=False)
            #h_embedding = self.embedding(input_sequence_padded.long())
            #h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))
            # h_lstm, _ = self.lstm(embedded)
            # h_lstm, _ = self.lstm2(h_lstm)
            # predicted = torch.nn.utils.rnn.pad_packed_sequence(h_lstm)[0].permute(1, 0, 2)
            # packed_redicted = utils.rnn.pack_padded_sequence(predicted,
            #                                                  seq_lengths,
            #                                                  batch_first=True,
            #                                                  enforce_sorted=False)
            # padded_packed_predicted = utils.rnn.pad_packed_sequence(packed_redicted)[0].permute(1, 0, 2)
            # h_lstm_atten = self.attention_layer(padded_packed_predicted)
            #
            # conc = self.relu(self.linear(h_lstm_atten))
            #
            # out = self.out(conc)
            # return F.log_softmax(out)

            # h_embedding = self.embedding(input_sequence_padded.long())
            # h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))
            # h_lstm, _ = self.lstm(h_embedding)
            # h_lstm, _ = self.lstm2(h_lstm)
            # h_lstm_atten = self.attention_layer(h_lstm)
            # conc = self.relu(self.linear(h_lstm_atten))
            # normalized_conc = self.normalize(conc)
            # out = self.out(normalized_conc)
            # return F.log_softmax(out)


        except Exception as ex:
            print(ex)
            print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)


            # output, hn = self.lstm(embedded)
            #normalized_output = self.normalize(output)
            #predicted_log_probabilities = F.log_softmax(self.linear(normalized_output), dim=1)
