import torch.nn as nn
import torch.nn.utils as utils
import torch.nn.functional as F
import torch

# from .attention import SCNNAttention
from utils.logger import *


log = get_logger(__name__)


class SCNN(nn.Module):
    def __init__(self, device, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(SCNN, self).__init__()
        # self.log = log
        #
        # self.categories_count = categories_count
        # self.embedding_size = embedding_size
        # self.hidden_size = hidden_size
        # self.classes_count = classes_count
        #
        # self.embedding = nn.Embedding(categories_count, embedding_size, 0)
        # self.gru = nn.GRU(embedding_size, hidden_size)  # Dan also added MultiHeadAtt
        # self.normalize = nn.BatchNorm1d(hidden_size)
        # self.linear = nn.Linear(hidden_size, classes_count)
        #
        self.device = device
        self.retry = 3

        #self.linear2 = nn.Linear(5922, 2) # TODO: make TFIDF vector size generic!

        # ## CNN of seq with CONV2D:
        # # Create word embeddings from the input words
        # self.embedding = nn.Embedding(45, 100, 0)
        # # Specify convolutions with filters of different sizes (fs)
        # self.convs = nn.ModuleList([nn.Conv2d(in_channels=1,
        #                                       out_channels=100,
        #                                       kernel_size=(fs, 100))
        #                             for fs in [1,2,3,4,5]])
        # # Add a fully connected layer for final predicitons
        # self.linear = nn.Linear(len([1,2,3,4,5])  * 100, 2)
        # # Drop some of the nodes to increase robustness in training
        # self.dropout = nn.Dropout(0.5)

        ## CNN of seq with CONVD:
        # Create word embeddings from the input words
        self.embedding = nn.Embedding(45+1, 100) # max_seq_len=10.000 # max seq = 520 first?
        # Specify convolutions with filters of different sizes (fs)
        self.convs = nn.ModuleList([nn.Conv1d(in_channels=100,
                                              out_channels=100,
                                              kernel_size=k)
                                    for k in [3]])
        # Add a fully connected layer for final predicitons
        self.linear = nn.Linear(len([3]) * 100, 2)
        # Drop some of the nodes to increase robustness in training
        #self.dropout = nn.Dropout(0.5)

    def forward(self, input_sequence):
        for try_count in range(self.retry):
            try:
                # embedding = self.embedding(input_sequence.data.long())
                # #embedding = self.embedding(input_sequence.data)
                # embedded = utils.rnn.PackedSequence(embedding, input_sequence.batch_sizes)
                # output, hn = self.gru(embedded)
                # output = hn.squeeze()
                # predicted_log_probabilities = F.log_softmax(self.linear(output), dim=1)

                #embedding = self.embedding(input_sequence.data.long())
                #embedding_new = self.embedding_new(input_sequence.data.long())
                #ret_new = self.fc_new(embedding_new)

                #predicted_log_probabilities2 = self.linear2(input_sequence.data.float())
                #predicted_log_probabilities2_softmax = F.log_softmax(predicted_log_probabilities2, dim=1)
                ##predicted_log_probabilities2_logsigmoid = F.logsigmoid(predicted_log_probabilities2)
                #return predicted_log_probabilities2_softmax


                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], 10000).to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1] ] = input_sequence

                # Get word embeddings and formt them for convolutions
                # Conv1d takes in (batch, channels, seq_len), but raw embedded is (batch, seq_len, channels)
                embedded = self.embedding(input_sequence_padded.long()).permute(0, 2, 1)

                # # convolutions and global max pooling and activation functions
                # conved = [F.relu(conv(embedded).permute(0, 2, 1).max(1)[0])
                #           for conv in self.convs]

                #Perform convolutions and apply activation functions
                #convolution and global max pooling
                conveds = [conv(embedded)
                           for conv in self.convs]

                # relued = [F.relu(conved.permute(0, 2, 1).max(1)[0])
                #           for conved in conveds]

                #pooleds = [conved[:,:,:input_sequence.shape[1]].sum(2).div(input_sequence.shape[1])
                #          for conved in conveds]

                pooleds = [F.avg_pool1d(conved[:,:,:input_sequence.shape[1]], input_sequence.shape[1]).squeeze(2)
                          for conved in conveds]

                relued = [F.relu(pooled)
                          for pooled in pooleds]

                # # Perform convolutions and apply activation functions
                # # convolution and global max pooling # F.relu(self.convs[0](embedded)[:,:,:input_sequence.shape[1]].sum(2))
                # conved = [F.relu(conv(embedded)[:,:,:input_sequence.shape[1]].sum(dim=2))
                #           for conv in self.convs]

                # Perform convolutions and apply activation functions
                # convolution and global max pooling # F.relu(self.convs[0](embedded)[:,:,:input_sequence.shape[1]].sum(2))
                # conved = [conv(embedded)[:,:,:input_sequence.shape[1]].sum(dim=2)
                #           for conv in self.convs]

                # relued = [F.relu(conv)
                #           for conv in conved]

                # Pooling layer to reduce dimensionality
                #pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2)
                #          for conv in conved]

                # Dropout layer
                #cat = self.dropout(torch.cat(conved, dim=1))

                return F.log_softmax(self.linear(torch.cat(relued, dim=1)))

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)
                continue


                # output, hn = self.lstm(embedded)
                #normalized_output = self.normalize(output)
                #predicted_log_probabilities = F.log_softmax(self.linear(normalized_output), dim=1)
